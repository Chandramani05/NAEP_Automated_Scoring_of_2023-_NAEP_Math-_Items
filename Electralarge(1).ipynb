{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1s5jQ-AEdRs7qGo8EZXRDrB1qncvMCdvF","timestamp":1686146470097}],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1Nj_3kwMEc4P20n0gdIajnqj1Eptob7f9","authorship_tag":"ABX9TyNoFjOgn4BlLy2qMYoC+n+U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers==4.28.0"],"metadata":{"id":"spFVLb2wHy6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"2KpldcGJKtyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets\n","!pip install -U git+https://github.com/huggingface/accelerate.git"],"metadata":{"id":"EvaCQ_XFY-XJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install nlpaug"],"metadata":{"id":"Obs5wmBETONZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HsTAUz6oHpVb"},"outputs":[],"source":["# Import libraries\n","\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","import seaborn as sns\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import initializers\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFDistilBertModel, DistilBertConfig"]},{"cell_type":"code","source":["df_main = pd.read_csv('/content/drive/MyDrive/NAEP_Comp/df_cleaned.csv')"],"metadata":{"id":"zCXvTP8eVF3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_1 = df_main[df_main['accession'] == 'VH525628']"],"metadata":{"id":"MXg0sQjhVJ0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"id":"E6DPVFZfVqw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import contractions\n","df_1['predict_from'] = df_1['predict_from'].fillna('N/A')\n","df_1['predict_from'] = df_1['predict_from'].apply(contractions.fix)"],"metadata":{"id":"E4iBgzWLVaSt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_1['predict_from']"],"metadata":{"id":"WtHaybgOV68F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List of unique accessions\n","unique_accessions = ['VH134067', 'VH139380', 'VH266015', 'VH266510', 'VH269384',\n","                     'VH271613', 'VH302907', 'VH304954', 'VH507804', 'VH525628']\n","\n","\n","\n","# Dictionary to store the dataframes\n","dfs = {}\n","\n","# Loop through the unique accessions\n","for accession in unique_accessions:\n","    # Create the dataframe name\n","    path = '/content/drive/MyDrive/NAEP_Comp/'\n","    df_name = 'df_' + accession\n","\n","    # Read the CSV file into a dataframe\n","    df = pd.read_csv(path + df_name + '.csv')\n","\n","    # Add the dataframe to the dictionary\n","    dfs[accession] = df"],"metadata":{"id":"kqnWEnZueNLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/NAEP_Comp/df_VH139380.csv')"],"metadata":{"id":"x5RJjF8WHxCr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.dropna(subset=['parsed_xml_v1'])"],"metadata":{"id":"i5ind1TwIV62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"45F34IU2Ijoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(df['predict_from_onestepall'], df['assigned_score'], test_size=0.2, stratify=df['assigned_score'], random_state=42)\n","X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, stratify = y_test, random_state=42)\n","X_test.shape, X_train.shape, X_valid.shape\n"],"metadata":{"id":"mlAwGkDLHp2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Our training data has   ', len(X_train.index), ' rows.')\n","print('Our validation data has ', len(X_valid.index), ' rows.')\n","print('Our test data has       ', len(X_test.index), ' rows.')"],"metadata":{"id":"HL-u6aW2JSpq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train -= 1\n","y_test -= 1\n","y_valid -= 1"],"metadata":{"id":"Nz9E1xqcLlVh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.unique(y_train\n","          )"],"metadata":{"id":"O4pH5tcwLv2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import transformers\n","import torch\n","import csv\n","\n","from datasets import Dataset,load_dataset, load_from_disk, load_metric\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import TrainingArguments, Trainer, AdamW\n","from sklearn.metrics import cohen_kappa_score\n","from torch.utils.data import DataLoader"],"metadata":{"id":"YXaTRmPUZCsP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import cohen_kappa_score\n","import os, sys, itertools, re"],"metadata":{"id":"R0FZWmUSQs-l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Change model to pretrain here\n","MODEL = \"google/electra-base-discriminator\""],"metadata":{"id":"lShaBxYjZC3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = dfs['VH139380']"],"metadata":{"id":"OZ7je2-Jeigv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Include columns that are important (features, labels, student_id)\n","df = df[[\"student_id\", \"predict_from\", \"score_to_predict\"]].set_index(\"student_id\").fillna(\"\")\n","df['labels'] = df['score_to_predict'] - 1\n","df.head()"],"metadata":{"id":"-7nS4etdZETU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset as d1"],"metadata":{"id":"9CMQOhWRVkF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import (ElectraForSequenceClassification,\n","                          ElectraTokenizerFast, EvalPrediction, InputFeatures,\n","                          Trainer, TrainingArguments, glue_compute_metrics)\n","from torch.utils.data import Dataset"],"metadata":{"id":"eE9VTFOrTo2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(eval_pred):\n","    logits, labels = eval_pred.predictions, eval_pred.label_ids\n","    preds = logits.argmax(axis=1)\n","    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n","    return {\"cohen_kappa\": kappa}"],"metadata":{"id":"MolDIXD-gdJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TrainerDataset(Dataset):\n","    def __init__(self, inputs, targets, tokenizer):\n","        self.inputs = inputs\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","\n","        # Tokenize the input\n","        self.tokenized_inputs = tokenizer(inputs, padding=True, max_length=80, truncation=True, add_special_tokens= True )   \n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        return InputFeatures(\n","            input_ids=self.tokenized_inputs['input_ids'][idx],\n","            token_type_ids=self.tokenized_inputs['token_type_ids'][idx],\n","            attention_mask=self.tokenized_inputs['attention_mask'][idx],\n","            label=self.targets[idx])      "],"metadata":{"id":"tzx9b6DWTzQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(train_dataset, eval_dataset, test_indexes, name, model) :\n","  # AdamW Training\n","  training_args = TrainingArguments(\n","    output_dir=\"./models/model_electra\",\n","    num_train_epochs=3,  # 1 (1 epoch gives slightly lower accuracy)\n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    save_total_limit = 2,\n","    save_strategy = 'no',\n","    load_best_model_at_end=False  # Make sure all batches are of equal size\n",")\n","  # Instantiate the Trainer class\n","  trainer = Trainer(\n","      model=model,\n","      args=training_args,\n","      train_dataset=train_dataset,\n","      eval_dataset=eval_dataset,\n","      compute_metrics=None)\n","  trainer.train()\n","  pred, actual, _ = trainer.predict(eval_dataset)\n","  pred_labels = np.argmax(pred, axis=1)\n","  results_df = pd.DataFrame(index=test_indexes)\n","  results_df['indexes'] = test_indexes\n","  results_df['True Labels'] = actual + 1\n","  results_df['PredictedValue'] = pred_labels + 1\n","  results_df.to_csv('/content/drive/MyDrive/NAEP_Comp/ElectraLarge/' + name + '.csv')\n","  kappa_score = cohen_kappa_score(actual, pred_labels, weights='quadratic')\n","  return kappa_score, model\n","\n","\n"],"metadata":{"id":"i_XHg-MhhahF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nlpaug.augmenter.word as naw\n","\n","def detect_minority_majority_classes(df, label_column):\n","\n","    class_counts = df[label_column].value_counts()\n","    minority_classes = class_counts[class_counts < class_counts.max()].index.tolist()\n","    majority_class = class_counts.idxmax()\n","    return minority_classes, majority_class\n","\n","def augment_minority_class_text(df, text_column, label_column):\n","    augmented_texts = []\n","    aug = naw.RandomWordAug(action=\"swap\")\n","    minority_classes, majority_class = detect_minority_majority_classes(df, label_column)\n","    print(df[label_column].value_counts())\n","    \n","    for minority_class in minority_classes:\n","        # Filter the dataframe to get only the minority class rows\n","        minority_df = df[df[label_column] == minority_class]\n","        majority_df = df[df[label_column] == majority_class]\n","        minority_count = len(minority_df)\n","        majority_count = len(majority_df)\n","        \n","        # Check if augmentation is required based on class imbalance\n","        if minority_count >= 0.6* majority_count:\n","            continue\n","\n","        # Calculate the number of augmentations required\n","        num_augmentations = int(0.6 * majority_count) - minority_count\n","        \n","        # Augment the text of the minority class\n","        while num_augmentations > 0:\n","            for text in minority_df[text_column]:\n","                augmented_text = aug.augment(text)\n","                if augmented_text:\n","                    augmented_texts.append((augmented_text[0], minority_class))  # Append augmented text with the minority class label\n","                    num_augmentations -= 1\n","                    if num_augmentations == 0:\n","                        break\n","\n","    # Create a new dataframe with augmented texts\n","    augmented_df = pd.DataFrame(augmented_texts, columns=[text_column, label_column])\n","    \n","    # Concatenate the augmented dataframe with the original dataframe\n","    augmented_df = pd.concat([df, augmented_df], ignore_index=True)\n","    print(augmented_df[label_column].value_counts())\n","    return augmented_df\n","\n"],"metadata":{"id":"99zaQoozTR5F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess(text):\n","    text=text.lower()\n","    # remove hyperlinks\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n","    #Replace &amp, &lt, &gt with &,<,> respectively\n","    text=text.replace(r'&amp;?',r'and')\n","    text=text.replace(r'&lt;',r'<')\n","    text=text.replace(r'&gt;',r'>')\n","    #remove hashtag sign\n","    #text=re.sub(r\"#\",\"\",text)   \n","    #remove mentions\n","    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n","    #text=re.sub(r\"@\",\"\",text)\n","    #remove non ascii chars\n","    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n","    #remove some puncts (except . ! ?)\n","    text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\n","    text=re.sub(r'[!]+','!',text)\n","    text=re.sub(r'[?]+','?',text)\n","    text=re.sub(r'[.]+','.',text)\n","    text=re.sub(r\"'\",\"\",text)\n","    text=re.sub(r\"\\(\",\"\",text)\n","    text=re.sub(r\"\\)\",\"\",text)\n","    \n","    text=\" \".join(text.split())\n","    return text"],"metadata":{"id":"1vfk5yhOS0ZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(df, name) :\n","  # Convert to dataset format\n","  # Include columns that are important (features, labels, student_id)\n","\n","  df = df[[\"student_id\", \"predict_from\", \"score_to_predict\"]].fillna(\"\")\n","  df['predict_from'] = df['predict_from'].apply(preprocess)\n","  df['predict_from'] = df['predict_from'].fillna('N/A')\n","  df['predict_from'] = df['predict_from'].apply(contractions.fix)\n","  df['labels'] = df['score_to_predict'] - 1\n","  df_train, df_valid = train_test_split(df, test_size = 0.2, stratify = df['labels'], random_state=11 )\n","  df_balanced = augment_minority_class_text(df_train, 'predict_from','labels')\n","  dataset_train = d1.from_pandas(df_balanced, preserve_index=False)\n","  dataset_valid = d1.from_pandas(df_valid, preserve_index=False)\n","  test_indexes = dataset_valid['student_id']\n","  model = ElectraForSequenceClassification.from_pretrained(MODEL, num_labels=df['labels'].nunique())\n","  model.cuda()\n","  tokenizer = ElectraTokenizerFast.from_pretrained(MODEL, do_lower_case=True)\n","  train_dataset = TrainerDataset(dataset_train[\"predict_from\"],\n","                               dataset_train[\"labels\"], tokenizer)\n","  eval_dataset = TrainerDataset(dataset_valid[\"predict_from\"],\n","                              dataset_valid[\"labels\"], tokenizer) \n","  training_args = TrainingArguments(\n","    output_dir=\"./models/model_electra\",\n","    num_train_epochs=5,  # 1 (1 epoch gives slightly lower accuracy)\n","    overwrite_output_dir=True,\n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    save_total_limit = 2,\n","    save_strategy = 'no',\n","    load_best_model_at_end=False ) # Make sure all batches are of equal size)\n","  # Instantiate the Trainer class\n","  trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics)\n","  trainer.train()\n","  pred, actual, _ = trainer.predict(eval_dataset)\n","  pred_labels = np.argmax(pred, axis=1)\n","  results_df = pd.DataFrame(index=test_indexes)\n","  results_df['indexes'] = test_indexes\n","  results_df['True Labels'] = actual + 1\n","  results_df['PredictedValue'] = pred_labels + 1\n","  results_df.to_csv('/content/drive/MyDrive/NAEP_Comp/ElectraLarge/' + name + '.csv')\n","  kappa_score = cohen_kappa_score(actual, pred_labels, weights='quadratic')\n","  model_save_name = name + '_b_electa.pth'\n","  path = model_save_name\n","  path2 = '/content/drive/MyDrive/NAEP_Comp/' + path\n","  torch.save(model.state_dict(), path)\n","  torch.save(model.state_dict(), path2)\n","  return kappa_score\n","\n"],"metadata":{"id":"w4RuiPXIghQg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"p6VghzR0T7Or"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_electra(df, name) :\n","  # Convert to dataset format\n","  # Include columns that are important (features, labels, student_id)\n","  MODEL2 =  \"google/electra-small-discriminator\"\n","  df = df[[\"student_id\", \"predict_from\", \"score_to_predict\"]].fillna(\"\")\n","  df['predict_from'] = df['predict_from'].apply(preprocess)\n","  df['labels'] = df['score_to_predict'] - 1\n","  df_train, df_valid = train_test_split(df, test_size = 0.2, stratify = df['labels'], random_state=11 )\n","  df_balanced = augment_minority_class_text(df_train, 'predict_from','labels')\n","  dataset_train = d1.from_pandas(df_train, preserve_index=False)\n","  dataset_valid = d1.from_pandas(df_valid, preserve_index=False)\n","  test_indexes = dataset_valid['student_id']\n","  model = ElectraForSequenceClassification.from_pretrained(MODEL2, num_labels=df['labels'].nunique())\n","  tokenizer = ElectraTokenizerFast.from_pretrained(MODEL2, do_lower_case=True)\n","  train_dataset = TrainerDataset(dataset_train[\"predict_from\"],\n","                               dataset_train[\"labels\"], tokenizer)\n","  eval_dataset = TrainerDataset(dataset_valid[\"predict_from\"],\n","                              dataset_valid[\"labels\"], tokenizer) \n","  training_args = TrainingArguments(\n","    output_dir=\"./models/model_electra2\",\n","    num_train_epochs=10,  # 1 (1 epoch gives slightly lower accuracy)\n","    overwrite_output_dir=True,\n","    evaluation_strategy=\"no\",\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    save_total_limit = 2,\n","    save_strategy = 'no',\n","    load_best_model_at_end=True ) # Make sure all batches are of equal size)\n","  # Instantiate the Trainer class\n","  trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=None)\n","  trainer.train()\n","  pred, actual, _ = trainer.predict(eval_dataset)\n","  pred_labels = np.argmax(pred, axis=1)\n","  print(pred_labels)\n","  results_df = pd.DataFrame(index=test_indexes)\n","  results_df['indexes'] = test_indexes\n","  results_df['True Labels'] = actual + 1\n","  results_df['PredictedValue'] = pred_labels + 1\n","  results_df.to_csv('/content/drive/MyDrive/NAEP_Comp/ElectraLarge/ElectraLarge/' + name + '.csv')\n","  kappa_score = cohen_kappa_score(actual, pred_labels, weights='quadratic')\n","  return kappa_score\n","\n"],"metadata":{"id":"CE6TpYsS2r3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import RobertaForSequenceClassification, RobertaTokenizer"],"metadata":{"id":"wSC6SC4kcmpG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"],"metadata":{"id":"a77VMUKAhC29"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize(batch):\n","    return tokenizer(batch[\"predict_from\"], padding=True, truncation=True, max_length=100, add_special_tokens=True)"],"metadata":{"id":"F6CtTmCVfC0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_first_string(value):\n","    if isinstance(value, list):\n","        if len(value) > 0 and isinstance(value[0], str):\n","            return value[0]   \n","    return value"],"metadata":{"id":"yWvVD9uN1uPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_roberta_large(df, name) :\n","  # Convert to dataset format\n","  # Include columns that are important (features, labels, student_id)\n","  MODEL =  \"roberta-base\"\n","  df = df[[\"student_id\", \"predict_from\", \"score_to_predict\"]].fillna(\"\")\n","  df['predict_from'] = df['predict_from'].apply(preprocess)\n","  df['label'] = df['score_to_predict'] - 1\n","  df_train, df_valid = train_test_split(df, test_size = 0.2, stratify = df['label'], random_state=11 )\n","  df_balanced = augment_minority_class_text(df_train, 'predict_from','label')\n","  df_balanced['predict_from'] = df_balanced['predict_from'].apply(extract_first_string)\n","  \n","  dataset_train = d1.from_pandas(df_balanced, preserve_index=False)\n","  dataset_valid = d1.from_pandas(df_valid, preserve_index=False)\n","  test_indexes = dataset_valid['student_id']\n","  model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=df['label'].nunique())\n","  tokenizer = RobertaTokenizer.from_pretrained(MODEL, do_lower_case=True)\n","  train_dataset = dataset_train.map(tokenize, batched=True, batch_size=len(dataset_train))\n","  val_dataset = dataset_valid.map(tokenize, batched=True, batch_size=len(dataset_valid))\n","  train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","  val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","\n","  training_args = TrainingArguments(\n","    output_dir=\"./models/model_electra2\",\n","    num_train_epochs=10,  # 1 (1 epoch gives slightly lower accuracy)\n","    overwrite_output_dir=True,\n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    save_total_limit = 2,\n","    save_strategy = 'epoch',\n","    load_best_model_at_end=False ) # Make sure all batches are of equal size)\n","  # Instantiate the Trainer class\n","  trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        compute_metrics=compute_metrics)\n","  trainer.train()\n","  pred, actual, _ = trainer.predict(val_dataset)\n","  pred_labels = np.argmax(pred, axis=1)\n","  print(pred_labels)\n","  results_df = pd.DataFrame(index=test_indexes)\n","  results_df['indexes'] = test_indexes\n","  results_df['True Labels'] = actual + 1\n","  results_df['PredictedValue'] = pred_labels + 1\n","  results_df.to_csv('/content/drive/MyDrive/NAEP_Comp/ElectraLarge/ElectraLarge/' + name + '.csv')\n","  kappa_score = cohen_kappa_score(actual, pred_labels, weights='quadratic')\n","  return kappa_score"],"metadata":{"id":"Otyiz_i5cj9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = dfs['VH525628']"],"metadata":{"id":"1fVk61uldAJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score = train_model(df_1, 'test_electra')\n","print(score)"],"metadata":{"id":"dM1zxXGMdHmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score"],"metadata":{"id":"ShLzMlL-upgS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = {}\n","for i, df in enumerate(dfs):\n","    name = unique_accessions[i]\n","    df = dfs[name]\n","    print(name)\n","    score = train_electra(df, name)\n","    results[name] = [score]  # Store score as a list\n","    print(score)\n","\n","# Create a DataFrame from the results\n","results_df = pd.DataFrame(results)\n","\n","# Save the DataFrame to a CSV file\n","results_df.to_csv('/content/drive/MyDrive/NAEP_Comp/ElectraLarge/ElectraLarge/Results_Cohen_' + str(name) + '.csv', index=False)"],"metadata":{"id":"SF8nZm3vi-Wf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9-7B_m5hcsVM"},"execution_count":null,"outputs":[]}]}