{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4251,"status":"ok","timestamp":1686708259222,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"DpQbI_Dcslqu"},"outputs":[],"source":["import os\n","import random\n","\n","import numpy as np\n","import torch\n","\n","SEED = 42\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(SEED)\n","random.seed(SEED)\n","os.environ['PYTHONHASHSEED'] = str(SEED)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29122,"status":"ok","timestamp":1686708288341,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"d9n0l-078oKf","outputId":"1eb7b607-c573-418b-f642-500dfc1b2a16"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers==4.28.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kng0jUximFPZ","executionInfo":{"status":"ok","timestamp":1686708300118,"user_tz":240,"elapsed":11781,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"051d9649-9c3d-4c6b-f97c-26fc7bdcdd31"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.28.0\n","  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.28.0\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1625,"status":"ok","timestamp":1686708319394,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"gta_Gwd7slqw"},"outputs":[],"source":["\n","\n","import torchtext\n","from torchtext.data import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator, GloVe, vocab\n","from torchtext.data.utils import get_tokenizer\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import set_seed, enable_full_determinism # For reproducibility\n","\n","from keras import layers\n","\n","\n","from typing import Tuple, Union, Callable, Dict, List\n","\n","\n","from timeit import default_timer as timer\n","\n","import numpy as np\n","import pandas as pd\n","import zipfile\n","\n","from IPython.display import display, HTML\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set_theme()\n","\n","import random\n","from tqdm import tqdm\n","\n","import multiprocessing\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"D2T9nxfvslqx","executionInfo":{"status":"ok","timestamp":1686708324609,"user_tz":240,"elapsed":367,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","import re            #for processing regular expressions\n","import math          #for basic maths operations\n","import string\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","import nltk\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","import nltk\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from string import punctuation\n","import re,string,unicodedata\n","from nltk import pos_tag\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import wordnet, stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import cross_val_score\n","from nltk.corpus import state_union\n","from nltk.tokenize import PunktSentenceTokenizer\n","from nltk.corpus import PlaintextCorpusReader\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from matplotlib import pyplot\n","from xgboost import XGBClassifier\n","\n","import pickle\n","from pickle import dump\n","from pickle import load\n","\n","import collections\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from math import sqrt\n","from numpy import hstack\n","from numpy import vstack\n","from numpy import asarray\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.svm import SVR\n","from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.ensemble import BaggingRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import ExtraTreesRegressor\n","\n","import sys\n","\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import GradientBoostingClassifier"]},{"cell_type":"code","source":["!pip install accelerate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BWkUzExnLr0o","executionInfo":{"status":"ok","timestamp":1686708331751,"user_tz":240,"elapsed":4691,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"86ac9dae-0554-4bb7-c286-4a244734e241"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting accelerate\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.20.3\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1686708331752,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"BVg38OwSslqy"},"outputs":[],"source":["import os\n","import re\n","import shutil\n","import string\n","\n","\n","from collections import Counter\n","\n","\n","import pandas as pd\n","import numpy as np\n","\n","import sklearn\n","\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7711,"status":"ok","timestamp":1686708339454,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"DKDwUfYHslqy"},"outputs":[],"source":["# List of unique accessions\n","unique_accessions = ['VH134067', 'VH139380', 'VH266015', 'VH266510', 'VH269384',\n","                     'VH271613', 'VH302907', 'VH304954', 'VH507804', 'VH525628']\n","\n","\n","\n","# Dictionary to store the dataframes\n","dfs = {}\n","\n","# Loop through the unique accessions\n","for accession in unique_accessions:\n","    # Create the dataframe name\n","    df_name = 'df_cleaned' + accession\n","    path = '/content/drive/MyDrive/NAEP_Comp/'\n","    # Read the CSV file into a dataframe\n","    df = pd.read_csv(path + df_name + '.csv')\n","\n","    # Add the dataframe to the dictionary\n","    dfs[accession] = df"]},{"cell_type":"code","source":["df_test = pd.read_csv('/content/drive/MyDrive/NAEP_Comp/df_test.csv')"],"metadata":{"id":"daVgknrlibQi","executionInfo":{"status":"ok","timestamp":1686708340187,"user_tz":240,"elapsed":774,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["dfs_test = {}\n","# Unique values in the 'accession' column\n","unique_accessions = df_test['accession'].unique().tolist()\n","# Loop through the unique accessions\n","for accession in unique_accessions:\n","    # Filter the dataframe for the current accession\n","    grouped_df = df_test[df_test['accession'] == accession]\n","    # Create the dataframe name\n","    path = '/content/drive/MyDrive/NAEP_Comp/Final Results'\n","    df_name = 'df_test' + accession\n","    dfs_test[accession] = grouped_df\n","\n","    # Save the dataframe as a CSV file\n","    grouped_df.to_csv(path + df_name + '.csv', index=False)"],"metadata":{"id":"-w50yQRkiIGe","executionInfo":{"status":"ok","timestamp":1686708346886,"user_tz":240,"elapsed":6702,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["df['predict_from']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06w0fbSGfkDo","executionInfo":{"status":"ok","timestamp":1686708378833,"user_tz":240,"elapsed":356,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"94103a3a-dab0-4f43-bd55-7b55676b9138"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        I believe im right because if you get the bige...\n","1        I know this might be right because of the x an...\n","2        Since Z is the smallest and W the biggest you ...\n","3        i know because x always comes first, then z is...\n","4        I know this answer is correct because if you a...\n","                               ...                        \n","16270    Because when you put them in this order you ar...\n","16271    If you were to add W and X and subsitute it fo...\n","16272                                       there in order\n","16273                                          i dont know\n","16274    Flip flopping each letter to create the least ...\n","Name: predict_from, Length: 16275, dtype: object"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHzCKJUo8iOf","executionInfo":{"status":"ok","timestamp":1686662333396,"user_tz":240,"elapsed":4,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"9f0b5c2c-f032-4fb4-c20a-82e02230c5b0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16275, 47)"]},"metadata":{},"execution_count":13}],"source":["df.shape"]},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBAJQ5M3X5qZ","executionInfo":{"status":"ok","timestamp":1686708387830,"user_tz":240,"elapsed":6055,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"da67a18c-7c4a-4697-9c60-87d42635629d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"]}]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-ZNqYE18iOf","executionInfo":{"status":"ok","timestamp":1686668664782,"user_tz":240,"elapsed":8,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"491eb3bb-e8d8-4a4c-dee1-5e0171f1b91c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    11659\n","2     3094\n","3     1522\n","Name: assigned_score, dtype: int64"]},"metadata":{},"execution_count":100}],"source":["df['assigned_score'].value_counts()"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"WiZyO-K38iOg","executionInfo":{"status":"ok","timestamp":1686708387830,"user_tz":240,"elapsed":8,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"outputs":[],"source":["# importing required libraries and modules\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.metrics import confusion_matrix, f1_score, precision_score\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"S5O-cwUU8iOi","executionInfo":{"status":"ok","timestamp":1686708420414,"user_tz":240,"elapsed":418,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["from transformers import AdamW\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoConfig"],"metadata":{"id":"83rIbsCsS7eI","executionInfo":{"status":"ok","timestamp":1686708428482,"user_tz":240,"elapsed":433,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ad4DC0QyhhMU","executionInfo":{"status":"ok","timestamp":1686708442373,"user_tz":240,"elapsed":12086,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"1aff3016-8c25-459d-e38f-e50b7e3f4046"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Collecting responses<0.19 (from datasets)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["from datasets import Dataset, DatasetDict\n"],"metadata":{"id":"KsFt4AHBm3wb","executionInfo":{"status":"ok","timestamp":1686708442831,"user_tz":240,"elapsed":469,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["model_nm = 'albert-base-v2'"],"metadata":{"id":"-pPTH6LjwDBG","executionInfo":{"status":"ok","timestamp":1686712075801,"user_tz":240,"elapsed":7,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8POVXIfwp-W","executionInfo":{"status":"ok","timestamp":1686712081409,"user_tz":240,"elapsed":3006,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"5cba918d-8065-4869-fafe-2ee5dae19a95"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","tokz = AutoTokenizer.from_pretrained(model_nm,df_lower_cases = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["74ac38569a654e32b65929a298c7ff80","8e4976109cfb478899fe7aa050d4f5b4","76288f40d54b4b798cf336e852777640","b58e66fde712460dadaa883edfc92e72","7f3da735e26d4e66873ec9bdfdec685a","d5818970356c4b82a1618309acf9adc5","3e652a690e2249708f53f16f2c4144cb","7da202e520f6415baa18d2e6825bba36","0d0f01238c5b4b2bbadc0e05022585d3","7cedc48a67da44b0a716fd8ecfcf5726","b5bef2680da54e38bae557a42035fdf3","e88cb1558dc247288450bd0d4eea1ec3","4c194bafa7b048d49c54806122d7d3f2","8b79f56950954b9dbb013b96f12269ef","189052d0d1864135a14338387da77ca3","85e7a4ada5bf41a9a9b9cf5bbc81bf64","2eafc184aa1049878cd3aabfb9174264","ec301288254648a5bdf372360310f35c","b5af8b14ba964fc9b4196e4c3a4bdd53","55631421e2164a008b27dd23e8c16c54","3fbd6549ddeb42d78b6676f374a710c9","c575197f1154494ea58d62272f24ee5d","4ca49499252e40868e4341b821639635","d5ddf51e3fec41b5975318d1eb1e0d1e","8dd2e31727b243a0a7f234ce59f6bd21","0a83024af29b4d00a71c4f3ebf390163","f86aa85681d84a88b98358b279c0266d","93b343a563fa4744abffeb10f418f16e","ca49bc7b633a4ebf87d6694ae10e7a83","4fabf410b4904b3685b384a360d7b360","ad7a95b794ee4d4da65638d46f661857","c86dc935add94bb79c1a158005eabc2f","25bf8d89478949b1b20bd5d178bcab81"]},"id":"oJVWhM0uwG84","executionInfo":{"status":"ok","timestamp":1686712085390,"user_tz":240,"elapsed":3985,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"58204dd0-8e25-4f67-e0ec-dd5a5dd631c7"},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ac38569a654e32b65929a298c7ff80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e88cb1558dc247288450bd0d4eea1ec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ca49499252e40868e4341b821639635"}},"metadata":{}}]},{"cell_type":"code","source":["def tok_function(x):\n","    return tokz.batch_encode_plus(x['predict_from'], truncation=True, padding = True, add_special_tokens = True, max_length = 220)"],"metadata":{"id":"_5CC4EhywMZj","executionInfo":{"status":"ok","timestamp":1686712088028,"user_tz":240,"elapsed":1,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","\n"],"metadata":{"id":"GpKcTQq5xTpi","executionInfo":{"status":"ok","timestamp":1686708450610,"user_tz":240,"elapsed":5,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainerCallback, Trainer, TrainingArguments"],"metadata":{"id":"sKHUm4jU-f8l","executionInfo":{"status":"ok","timestamp":1686708450610,"user_tz":240,"elapsed":4,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["bs = 32 \n","epochs = 4\n","lr = 4e-6\n","\n","args = TrainingArguments(output_dir = \"./models/model_electra2_prompt_1\",\n","  num_train_epochs=4 ,# 1 (1 epoch gives slightly lower accuracy)\n","  overwrite_output_dir=True,\n","  evaluation_strategy=\"epoch\",\n","  per_device_train_batch_size=32,\n","  per_device_eval_batch_size=32,\n","  save_total_limit = 2,\n","  save_strategy = 'no',\n","  load_best_model_at_end=False)"],"metadata":{"id":"G_FUbAXFxhZn","executionInfo":{"status":"ok","timestamp":1686712095206,"user_tz":240,"elapsed":2,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","## to see the relation between the input and label \n","## I'll use the Pearson's coefficient\n","\n","def corr(x,y): return np.corrcoef(x,y)[0][1]\n","def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}"],"metadata":{"id":"OoolkWPjxjvY","executionInfo":{"status":"ok","timestamp":1686712097557,"user_tz":240,"elapsed":1,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import cohen_kappa_score\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    quadratic_cohen_kappa = cohen_kappa_score(predictions, labels, weights='quadratic')\n","    return {'quadratic_cohen_kappa': quadratic_cohen_kappa}\n"],"metadata":{"id":"iFT-5rJfybhD","executionInfo":{"status":"ok","timestamp":1686708460561,"user_tz":240,"elapsed":3,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def save_results(predictions, true_labels, dds) : \n","    # Get the required columns from dds['test']\n","  columns = ['srace10', 'dsex', 'accom2', 'iep', 'lep', 'label', 'predict_from','student_id']\n","  data = {col: dds['test'][col] for col in columns}\n","  name = dds['test']['accession']\n","  student_id = dds['test']['student_id']\n","  prompt = name[0]\n","  #if prompt is None : prompt = \"prompt4\"\n","  #path = \"/content/drive/MyDrive/NAEP_Comp/Alberta/\" + prompt + \".csv\"\n","  # Create a DataFrame\n","  result = pd.DataFrame(data)\n","  result['student_id'] = student_id\n","  result['predicted'] = predictions\n","  result['true_label'] = true_labels\n"," \n","  #result.to_csv(path)\n","  return result "],"metadata":{"id":"uWoxAYfTHfIS","executionInfo":{"status":"ok","timestamp":1686708462601,"user_tz":240,"elapsed":4,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def train_model(df) :\n","  ds = Dataset.from_pandas(df)\n","  tok_ds=ds.map(tok_function, batched=True)\n","  tok_ds = tok_ds.rename_column(\"assigned_score\", \"label\")\n","  dds = tok_ds.train_test_split(0.1,seed=42)\n","  model = AutoModelForSequenceClassification.from_pretrained(model_nm,num_labels=df['assigned_score'].nunique()+1).to(device)\n","  model.cuda()\n","  trainer = Trainer(model, args, train_dataset = dds['train'],\n","                 eval_dataset = dds['test'], tokenizer = tokz, compute_metrics=compute_metrics )\n","  trainer.train()\n","  preds = trainer.predict(dds['test'])\n","  predictions = preds.predictions.argmax(axis=1)  # Assuming it's a classification task\n","  true_labels = preds.label_ids\n","  result = save_results(predictions, true_labels, dds)\n","  return cohen_kappa_score(predictions, true_labels, weights='quadratic'), result, trainer\n"],"metadata":{"id":"v2x3lETS4buv","executionInfo":{"status":"ok","timestamp":1686708512157,"user_tz":240,"elapsed":2,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords"],"metadata":{"id":"7NQJLuYjX4Q5","executionInfo":{"status":"ok","timestamp":1686708466168,"user_tz":240,"elapsed":1,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["df = dfs['VH525628']"],"metadata":{"id":"w76yRDRvH2yy","executionInfo":{"status":"ok","timestamp":1686708477968,"user_tz":240,"elapsed":1,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["import contractions\n","def preprocess(text):\n","    text=text.lower()\n","    # remove hyperlinks\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n","    #Replace &amp, &lt, &gt with &,<,> respectively\n","    text=text.replace(r'&amp;?',r'and')\n","    text=text.replace(r'&lt;',r'<')\n","    text=text.replace(r'&gt;',r'>')\n","    #remove hashtag sign\n","    text=re.sub(r\"#\",\"\",text)   \n","    #remove mentions\n","    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n","    #text=re.sub(r\"@\",\"\",text)\n","    #remove non ascii chars\n","    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n","    #remove some puncts (except . ! ?)\n","    text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\n","    text=re.sub(r'[!]+','!',text)\n","    text=re.sub(r'[?]+','?',text)\n","    text=re.sub(r'[.]+','.',text)\n","    text=re.sub(r\"'\",\"\",text)\n","    text=re.sub(r\"\\(\",\"\",text)\n","    text=re.sub(r\"\\)\",\"\",text)\n","    \n","    text=\" \".join(text.split())\n","    return text\n","\n","def remove_stopwords(df):\n","    # Download stopwords if not already downloaded\n","    nltk.download('stopwords')\n","\n","    # Get the list of stopwords\n","    stop_words = set(stopwords.words('english'))\n","\n","    # Apply stopwords removal to the 'predict_from' column\n","    df['predict_from'] = df['predict_from'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n","\n","    return df    \n","\n","df['predict_from'] =     df['predict_from'].apply(preprocess)\n","df['predict_from'] = df['predict_from'].apply(contractions.fix)\n","#df = remove_stopwords(df)\n"],"metadata":{"id":"N2q4QBdMLWbb","executionInfo":{"status":"ok","timestamp":1686681957127,"user_tz":240,"elapsed":915,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"252ce9e8-7a63-46e2-b53a-769b9c092471"},"execution_count":294,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["df['predict_from']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfZv1fePYP3z","executionInfo":{"status":"ok","timestamp":1686712103517,"user_tz":240,"elapsed":358,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"d20d7e12-82a5-4b6f-a10f-54313e910aba"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        I believe im right because if you get the bige...\n","1        I know this might be right because of the x an...\n","2        Since Z is the smallest and W the biggest you ...\n","3        i know because x always comes first, then z is...\n","4        I know this answer is correct because if you a...\n","                               ...                        \n","16270    Because when you put them in this order you ar...\n","16271    If you were to add W and X and subsitute it fo...\n","16272                                       there in order\n","16273                                          i dont know\n","16274    Flip flopping each letter to create the least ...\n","Name: predict_from, Length: 16275, dtype: object"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()"],"metadata":{"id":"pLlVenl8svBy","executionInfo":{"status":"ok","timestamp":1686712106683,"user_tz":240,"elapsed":4,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["tr, result, trainer = train_model(df)"],"metadata":{"id":"jRAO3U1EH9pr","executionInfo":{"status":"error","timestamp":1686713968601,"user_tz":240,"elapsed":1860499,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6d8a0967fc3f45aeaf8d1938fdad1aee","a25b0b6814b84a2393aad4eb68ca4028","6a233144008e4f14b89f1beb8a726376","8389bcc4ba92416f8d6e9ff480a77565","85684b4207c14818a61af5e99091566e","abfc5599cede495485882192b05ca1f1","f0ffdc8f889a4fc8a1f6340905a28bc7","f1054c398bc149ed856303080c779ac1","76251b2f815745ef96cf5cfdb54e6247","a37ae52ddf4642ec86c633bb1eac26ef","7ecd8ea34fe24a2993f8ceb1050b1d3e","0c4b207d9743483c9167e43d75ec3cfa","aaf1c666ea1c4380bc13b7bcac7dbd07","ccfa7def232e40c6a18194291e13f9f0","df9c0d4ba068499793e79e5c0c344ad6","b5ef2cc9035646a689de2c26757a302b","ea5f6745f3464abd9aa0976496f05803","172313a2bf7248908c3ad2e38d746be7","84081067b6f2433fbcc909b59d9a14e2","e3479b2ea85e4120858372f48097d3ea","f4b1bb96a4f441b482343654e681f901","a9e4d8d0c6b44f3c9c8fc8c2572689ce"]},"outputId":"0c4ba540-2610-4360-d6d1-5c311317db0b"},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/16275 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d8a0967fc3f45aeaf8d1938fdad1aee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4b207d9743483c9167e43d75ec3cfa"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias']\n","- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1506' max='1832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1506/1832 30:53 < 06:41, 0.81 it/s, Epoch 3.29/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Quadratic Cohen Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.636307</td>\n","      <td>0.512531</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.642100</td>\n","      <td>0.582867</td>\n","      <td>0.505774</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.542000</td>\n","      <td>0.560626</td>\n","      <td>0.614655</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92mtrain_model\u001b[0m:\u001b[94m10\u001b[0m                                                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1662 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1929\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1926 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1927 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1929 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1930 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1931 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1932 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2717\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2714 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[0m             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2715 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.deepspeed.backward(loss)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2716 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2717 \u001b[2m│   │   │   \u001b[0mloss.backward()                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2718 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2719 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach()                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2720 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyboardInterrupt\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_model</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 │   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1929</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1926 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1927 │   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1928 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1929 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1930 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1931 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1932 │   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2717</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2714 │   │   │   # loss gets scaled under gradient_accumulation_steps in deepspeed</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2715 │   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed.backward(loss)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2716 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2717 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2718 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2719 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2720 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"xsiln5WL03sl","executionInfo":{"status":"ok","timestamp":1686698465004,"user_tz":240,"elapsed":1137,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"f74f7d0e-2f4e-4ef5-f5fc-5a095b23cf06"},"execution_count":410,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      srace10  dsex  accom2  iep  lep  label  \\\n","0         3.0   1.0     1.0  1.0  1.0      2   \n","1         3.0   1.0     2.0  2.0  2.0      3   \n","2         4.0   1.0     2.0  2.0  2.0      3   \n","3         2.0   1.0     2.0  2.0  2.0      2   \n","4         1.0   2.0     2.0  2.0  2.0      3   \n","...       ...   ...     ...  ...  ...    ...   \n","1811      1.0   1.0     2.0  2.0  2.0      3   \n","1812      1.0   2.0     2.0  2.0  2.0      2   \n","1813      1.0   2.0     2.0  2.0  2.0      2   \n","1814      4.0   1.0     2.0  2.0  2.0      3   \n","1815      1.0   2.0     2.0  2.0  2.0      3   \n","\n","                                           predict_from  student_id  \\\n","0                                                     8  v65cpQmUxp   \n","1                                  you can count by 3\"s  o9SYEexR1m   \n","2                                 Add three every time.  NjlNf7rrp8   \n","3                                                   5+3  7e8OhouIvQ   \n","4                                                    +3  0UoYogCzaD   \n","...                                                 ...         ...   \n","1811                                              add 3  gm8oBiCJKl   \n","1812                                                  2  S5vw4CurMa   \n","1813  You add up from the first  number in the pattern.  1p9xlu401K   \n","1814  You could do 5 then count up to 8 then the ans...  6p0uieBgtE   \n","1815                                              add 3  rX6x4b9WIv   \n","\n","      predicted  true_label  \n","0             1           2  \n","1             3           3  \n","2             3           3  \n","3             2           2  \n","4             3           3  \n","...         ...         ...  \n","1811          3           3  \n","1812          2           2  \n","1813          1           2  \n","1814          3           3  \n","1815          3           3  \n","\n","[1816 rows x 10 columns]"],"text/html":["\n","  <div id=\"df-72c43f88-0707-4761-99b3-b03beec25eba\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>srace10</th>\n","      <th>dsex</th>\n","      <th>accom2</th>\n","      <th>iep</th>\n","      <th>lep</th>\n","      <th>label</th>\n","      <th>predict_from</th>\n","      <th>student_id</th>\n","      <th>predicted</th>\n","      <th>true_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>v65cpQmUxp</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>you can count by 3\"s</td>\n","      <td>o9SYEexR1m</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>Add three every time.</td>\n","      <td>NjlNf7rrp8</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2</td>\n","      <td>5+3</td>\n","      <td>7e8OhouIvQ</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>+3</td>\n","      <td>0UoYogCzaD</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1811</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>add 3</td>\n","      <td>gm8oBiCJKl</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1812</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>S5vw4CurMa</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1813</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2</td>\n","      <td>You add up from the first  number in the pattern.</td>\n","      <td>1p9xlu401K</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1814</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>You could do 5 then count up to 8 then the ans...</td>\n","      <td>6p0uieBgtE</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1815</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>add 3</td>\n","      <td>rX6x4b9WIv</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1816 rows × 10 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72c43f88-0707-4761-99b3-b03beec25eba')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-72c43f88-0707-4761-99b3-b03beec25eba button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-72c43f88-0707-4761-99b3-b03beec25eba');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":410}]},{"cell_type":"code","source":["result[result['true_label'] != result['label']]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"id":"INF_Fpi_fE9M","executionInfo":{"status":"ok","timestamp":1686698468681,"user_tz":240,"elapsed":387,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"cf964569-c944-4f75-f7ca-9e21f9ca575a"},"execution_count":411,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [srace10, dsex, accom2, iep, lep, label, predict_from, student_id, predicted, true_label]\n","Index: []"],"text/html":["\n","  <div id=\"df-67f9fa9c-3877-4b05-99e4-c8482cd14a11\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>srace10</th>\n","      <th>dsex</th>\n","      <th>accom2</th>\n","      <th>iep</th>\n","      <th>lep</th>\n","      <th>label</th>\n","      <th>predict_from</th>\n","      <th>student_id</th>\n","      <th>predicted</th>\n","      <th>true_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67f9fa9c-3877-4b05-99e4-c8482cd14a11')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-67f9fa9c-3877-4b05-99e4-c8482cd14a11 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-67f9fa9c-3877-4b05-99e4-c8482cd14a11');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":411}]},{"cell_type":"code","source":["df_1 = result.copy()"],"metadata":{"id":"kvyhobY2g0ta","executionInfo":{"status":"ok","timestamp":1686698487519,"user_tz":240,"elapsed":7,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":412,"outputs":[]},{"cell_type":"code","source":["df_1.columns = df_1.columns.str.replace('_x', '').str.replace('_y', '')\n","\n","# Drop duplicate columns\n","df_1 = df_1.loc[:, ~df_1.columns.duplicated()]\n","\n","df_1.drop(['predict_from','label'], axis =1, inplace=True)\n","df_1.rename({'true_label': 'true_labels'})\n","df_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"XRC2fbchgKDM","executionInfo":{"status":"ok","timestamp":1686698491418,"user_tz":240,"elapsed":688,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"be52a2cb-8e7b-49eb-d2e9-1b6b4c42896c"},"execution_count":413,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      srace10  dsex  accom2  iep  lep  student_id  predicted  true_label\n","0         3.0   1.0     1.0  1.0  1.0  v65cpQmUxp          1           2\n","1         3.0   1.0     2.0  2.0  2.0  o9SYEexR1m          3           3\n","2         4.0   1.0     2.0  2.0  2.0  NjlNf7rrp8          3           3\n","3         2.0   1.0     2.0  2.0  2.0  7e8OhouIvQ          2           2\n","4         1.0   2.0     2.0  2.0  2.0  0UoYogCzaD          3           3\n","...       ...   ...     ...  ...  ...         ...        ...         ...\n","1811      1.0   1.0     2.0  2.0  2.0  gm8oBiCJKl          3           3\n","1812      1.0   2.0     2.0  2.0  2.0  S5vw4CurMa          2           2\n","1813      1.0   2.0     2.0  2.0  2.0  1p9xlu401K          1           2\n","1814      4.0   1.0     2.0  2.0  2.0  6p0uieBgtE          3           3\n","1815      1.0   2.0     2.0  2.0  2.0  rX6x4b9WIv          3           3\n","\n","[1816 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-e11f5e84-b50d-435a-be51-b904f26c80f4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>srace10</th>\n","      <th>dsex</th>\n","      <th>accom2</th>\n","      <th>iep</th>\n","      <th>lep</th>\n","      <th>student_id</th>\n","      <th>predicted</th>\n","      <th>true_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>v65cpQmUxp</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>o9SYEexR1m</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>NjlNf7rrp8</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7e8OhouIvQ</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0UoYogCzaD</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1811</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>gm8oBiCJKl</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1812</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>S5vw4CurMa</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1813</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1p9xlu401K</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1814</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>6p0uieBgtE</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1815</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>rX6x4b9WIv</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1816 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e11f5e84-b50d-435a-be51-b904f26c80f4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e11f5e84-b50d-435a-be51-b904f26c80f4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e11f5e84-b50d-435a-be51-b904f26c80f4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":413}]},{"cell_type":"code","source":["df_1 = df_1.rename(columns={'true_label': 'true_labels'})\n","df_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"dNV9L_Vdhckf","executionInfo":{"status":"ok","timestamp":1686698495739,"user_tz":240,"elapsed":381,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"dcf2f8c4-b317-4042-d85e-57e9b1b8ba16"},"execution_count":414,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      srace10  dsex  accom2  iep  lep  student_id  predicted  true_labels\n","0         3.0   1.0     1.0  1.0  1.0  v65cpQmUxp          1            2\n","1         3.0   1.0     2.0  2.0  2.0  o9SYEexR1m          3            3\n","2         4.0   1.0     2.0  2.0  2.0  NjlNf7rrp8          3            3\n","3         2.0   1.0     2.0  2.0  2.0  7e8OhouIvQ          2            2\n","4         1.0   2.0     2.0  2.0  2.0  0UoYogCzaD          3            3\n","...       ...   ...     ...  ...  ...         ...        ...          ...\n","1811      1.0   1.0     2.0  2.0  2.0  gm8oBiCJKl          3            3\n","1812      1.0   2.0     2.0  2.0  2.0  S5vw4CurMa          2            2\n","1813      1.0   2.0     2.0  2.0  2.0  1p9xlu401K          1            2\n","1814      4.0   1.0     2.0  2.0  2.0  6p0uieBgtE          3            3\n","1815      1.0   2.0     2.0  2.0  2.0  rX6x4b9WIv          3            3\n","\n","[1816 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-07c08f3d-0ce9-45f9-8272-91dfd55d65a8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>srace10</th>\n","      <th>dsex</th>\n","      <th>accom2</th>\n","      <th>iep</th>\n","      <th>lep</th>\n","      <th>student_id</th>\n","      <th>predicted</th>\n","      <th>true_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>v65cpQmUxp</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>o9SYEexR1m</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>NjlNf7rrp8</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7e8OhouIvQ</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0UoYogCzaD</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1811</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>gm8oBiCJKl</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1812</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>S5vw4CurMa</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1813</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1p9xlu401K</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1814</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>6p0uieBgtE</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1815</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>rX6x4b9WIv</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1816 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07c08f3d-0ce9-45f9-8272-91dfd55d65a8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-07c08f3d-0ce9-45f9-8272-91dfd55d65a8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-07c08f3d-0ce9-45f9-8272-91dfd55d65a8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":414}]},{"cell_type":"code","source":["true_labels = df_1['true_labels']\n","pred = df_1['predicted']"],"metadata":{"id":"xmCU4NCZheOP","executionInfo":{"status":"ok","timestamp":1686698499851,"user_tz":240,"elapsed":568,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":415,"outputs":[]},{"cell_type":"code","source":["cohen_kappa_score(pred, true_labels, weights = 'quadratic')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ry5_sMWRh3yc","executionInfo":{"status":"ok","timestamp":1686698503015,"user_tz":240,"elapsed":3,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"6fbdc604-80eb-4c24-88a0-95d96eca6992"},"execution_count":416,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8177325888166498"]},"metadata":{},"execution_count":416}]},{"cell_type":"code","source":["\n","df_1.to_csv('/content/drive/MyDrive/NAEP_Comp/Final Results/result_VH139380_distill.csv', index = False)\n","df_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"WPXq6Uveh8KJ","executionInfo":{"status":"ok","timestamp":1686698863387,"user_tz":240,"elapsed":1217,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"68a428b0-d56f-47b5-8bd8-e0bcc04da850"},"execution_count":418,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      srace10  dsex  accom2  iep  lep  student_id  predicted  true_labels\n","0         3.0   1.0     1.0  1.0  1.0  v65cpQmUxp          1            2\n","1         3.0   1.0     2.0  2.0  2.0  o9SYEexR1m          3            3\n","2         4.0   1.0     2.0  2.0  2.0  NjlNf7rrp8          3            3\n","3         2.0   1.0     2.0  2.0  2.0  7e8OhouIvQ          2            2\n","4         1.0   2.0     2.0  2.0  2.0  0UoYogCzaD          3            3\n","...       ...   ...     ...  ...  ...         ...        ...          ...\n","1811      1.0   1.0     2.0  2.0  2.0  gm8oBiCJKl          3            3\n","1812      1.0   2.0     2.0  2.0  2.0  S5vw4CurMa          2            2\n","1813      1.0   2.0     2.0  2.0  2.0  1p9xlu401K          1            2\n","1814      4.0   1.0     2.0  2.0  2.0  6p0uieBgtE          3            3\n","1815      1.0   2.0     2.0  2.0  2.0  rX6x4b9WIv          3            3\n","\n","[1816 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-cf3fbe38-266c-4bde-8aad-e4eea0f598db\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>srace10</th>\n","      <th>dsex</th>\n","      <th>accom2</th>\n","      <th>iep</th>\n","      <th>lep</th>\n","      <th>student_id</th>\n","      <th>predicted</th>\n","      <th>true_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>v65cpQmUxp</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>o9SYEexR1m</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>NjlNf7rrp8</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7e8OhouIvQ</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0UoYogCzaD</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1811</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>gm8oBiCJKl</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1812</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>S5vw4CurMa</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1813</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1p9xlu401K</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1814</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>6p0uieBgtE</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1815</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>rX6x4b9WIv</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1816 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf3fbe38-266c-4bde-8aad-e4eea0f598db')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cf3fbe38-266c-4bde-8aad-e4eea0f598db button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cf3fbe38-266c-4bde-8aad-e4eea0f598db');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":418}]},{"cell_type":"code","source":["df_test = dfs_test['VH139380']"],"metadata":{"id":"AaEU-DMAim2M","executionInfo":{"status":"ok","timestamp":1686698883980,"user_tz":240,"elapsed":1090,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":419,"outputs":[]},{"cell_type":"code","source":["ds_test = Dataset.from_pandas(df_test)\n","tok_ds=ds_test.map(tok_function, batched=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["9141c1c72e494250823c4a0713acd3c7","6e954b0a47bb4cfa9bf86d42a80cab2c","3161252546e04f22a9b6a3fde2807c52","0d5c857289ba4e7eae167cb924fd245f","a6e9ae3ca0f94ace8a0c9ebce1b7e553","d1cce93214174ed798af8a3d3ce4b315","7cbb58badf1f406181641bac69dc935c","56293ee565f24ed090571d515e4d94a9","8a21c0ddc9af4871ae65fd10cdd5fe84","f3075e14f3194feda746bc256342a7cb","0ca3fc84c0494c6884e7c8c7dd0b0f7e"]},"id":"_N3WXqIqireS","executionInfo":{"status":"ok","timestamp":1686698884606,"user_tz":240,"elapsed":4,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"2a92378d-0276-4c59-9b24-cd2eeb64a5a0"},"execution_count":420,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2018 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9141c1c72e494250823c4a0713acd3c7"}},"metadata":{}}]},{"cell_type":"code","source":["preds = trainer.predict(tok_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"q1LUL0M1itUQ","executionInfo":{"status":"ok","timestamp":1686698895287,"user_tz":240,"elapsed":8205,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"05369bb8-de54-4182-8fbf-96131f5e74a6"},"execution_count":421,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","source":["preds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPqfcRu0s98C","executionInfo":{"status":"ok","timestamp":1686692424970,"user_tz":240,"elapsed":6,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"e1567556-44f1-41dd-ba98-e309c3408a29"},"execution_count":364,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PredictionOutput(predictions=array([[-4.962526 ,  3.3283684,  2.941154 , -3.5231504],\n","       [-5.2111   ,  3.088646 ,  3.3304822, -3.4770365],\n","       [-5.044511 ,  3.0665832,  2.9279046, -3.1136398],\n","       ...,\n","       [-3.6554577,  4.3942165,  0.7045688, -3.319344 ],\n","       [-4.79225  ,  3.8307502,  2.2912433, -3.5974665],\n","       [-4.8740416,  3.1707036,  2.8631444, -3.2938762]], dtype=float32), label_ids=None, metrics={'test_runtime': 23.3008, 'test_samples_per_second': 117.722, 'test_steps_per_second': 3.691})"]},"metadata":{},"execution_count":364}]},{"cell_type":"code","source":["pred_labels =  preds.predictions.argmax(axis=1) \n","pred_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_wPQk2_s8hc","executionInfo":{"status":"ok","timestamp":1686698897738,"user_tz":240,"elapsed":6,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"6a17021b-1aa5-4ace-fa71-dc6bb1c29f82"},"execution_count":422,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 2, 3, ..., 2, 3, 2])"]},"metadata":{},"execution_count":422}]},{"cell_type":"code","source":["df_test_score = df_test.copy()\n","df_test_score['score_to_predict'] = pred_labels \n","df_test_score['score_to_predict'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-6cS-KesxUv","executionInfo":{"status":"ok","timestamp":1686698897739,"user_tz":240,"elapsed":4,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"94da764c-3817-450a-ba15-c01894b559d2"},"execution_count":423,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3    889\n","2    832\n","1    297\n","Name: score_to_predict, dtype: int64"]},"metadata":{},"execution_count":423}]},{"cell_type":"code","source":["df['score_to_predict'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aznBiIFstSoT","executionInfo":{"status":"ok","timestamp":1686698905330,"user_tz":240,"elapsed":582,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"26f2e465-c717-4e67-e5fb-029182df85b2"},"execution_count":425,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3    7598\n","2    7442\n","1    3117\n","Name: score_to_predict, dtype: int64"]},"metadata":{},"execution_count":425}]},{"cell_type":"code","source":["df_test_score.to_csv('/content/drive/MyDrive/NAEP_Comp/Final Results/test_result_VH139380.csv', index = False)"],"metadata":{"id":"y1coBnbitSJ0","executionInfo":{"status":"ok","timestamp":1686698906675,"user_tz":240,"elapsed":9,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":426,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, df, maxlen, with_labels=True, bert_model='albert-base-v2'):\n","\n","        self.data = df  # pandas dataframe\n","        #Initialize the tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n","\n","        self.maxlen = maxlen\n","        self.with_labels = with_labels \n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        # Selecting sentence1 and sentence2 at the specified index in the data frame\n","        sent1 = str(self.data.loc[index, 'predict_from'])\n","\n","        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n","        encoded_pair = self.tokenizer(sent1 ,\n","                                      padding='max_length',  # Pad to max_length\n","                                      truncation=True,  # Truncate to max_length\n","                                      max_length=self.maxlen,  \n","                                      return_tensors='pt')  # Return torch.Tensor objects\n","        \n","        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n","        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n","        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n","\n","        if self.with_labels:  # True if the dataset has labels\n","            label = self.data.loc[index, 'score_to_predict'] - 1\n","            return token_ids, attn_masks, token_type_ids, label  \n","        else:\n","            return token_ids, attn_masks, token_type_ids"],"metadata":{"id":"7RJlbS3LSqA-","executionInfo":{"status":"ok","timestamp":1686669111390,"user_tz":240,"elapsed":3,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":119,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Dataset\n","from torch.cuda.amp import autocast, GradScaler"],"metadata":{"id":"Me880kOlTHiC","executionInfo":{"status":"ok","timestamp":1686668869212,"user_tz":240,"elapsed":2,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["class SentencePairClassifier(nn.Module):\n","\n","    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n","        super(SentencePairClassifier, self).__init__()\n","        #  Instantiating BERT-based model object\n","        self.bert_layer = AutoModel.from_pretrained(bert_model)\n","\n","        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n","        if bert_model == \"albert-base-v2\":  # 12M parameters\n","            hidden_size = 768\n","        elif bert_model == \"albert-large-v2\":  # 18M parameters\n","            hidden_size = 1024\n","        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n","            hidden_size = 2048\n","        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n","            hidden_size = 4096\n","        elif bert_model == \"bert-base-uncased\": # 110M parameters\n","            hidden_size = 768\n","\n","        # Freeze bert layers and only train the classification layer weights\n","        if freeze_bert:\n","            for p in self.bert_layer.parameters():\n","                p.requires_grad = False\n","\n","        # Classification layer\n","        #self.cls_layer = nn.Linear(hidden_size, 3)\n","        self.classifier = nn.Linear(hidden_size,1)\n","\n","        #self.dropout = nn.Dropout(0.1)\n","\n","     # run in mixed precision\n","    def forward(self, input_ids, attn_masks, token_type_ids):\n","        '''\n","        Inputs:\n","            -input_ids : Tensor  containing token ids\n","            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n","            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n","        '''\n","\n","        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n","        output= self.bert_layer(input_ids, attn_masks, token_type_ids)\n","        #print(output[0])\n","\n","        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\n","        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\n","        # objective during pre-training.\n","        #pooler_output = pooler_output.view(pooler_output.size(0), -1)\n","        logits = self.classifier(output[0])\n","\n","        return logits"],"metadata":{"id":"XFkG4qwAS9nJ","executionInfo":{"status":"ok","timestamp":1686671861355,"user_tz":240,"elapsed":347,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":199,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","    \"\"\" Set all seeds to make results reproducible \"\"\"\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    \n","\n","def evaluate_loss(net, device, criterion, dataloader):\n","    net.eval()\n","\n","    mean_loss = 0\n","    count = 0\n","\n","    with torch.no_grad():\n","        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n","            seq, attn_masks, token_type_ids, labels = \\\n","                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n","            logits = net(seq, attn_masks, token_type_ids)\n","            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n","            count += 1\n","\n","    return mean_loss / count"],"metadata":{"id":"0aUOWB6xTOFj","executionInfo":{"status":"ok","timestamp":1686670605551,"user_tz":240,"elapsed":2,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":173,"outputs":[]},{"cell_type":"code","source":["def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n","\n","    best_loss = np.Inf\n","    best_ep = 1\n","    nb_iterations = len(train_loader)\n","    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n","    iters = []\n","    train_losses = []\n","    val_losses = []\n","\n","    scaler = GradScaler()\n","\n","    for ep in range(epochs):\n","\n","        net.train()\n","        running_loss = 0.0\n","        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n","\n","            # Converting to cuda tensors\n","            seq, attn_masks, token_type_ids, labels = \\\n","                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n","          \n","            #print(seq, attn_masks, token_type_ids, labels)\n","            # Enables autocasting for the forward pass (model + loss)\n","            print(labels.shape)\n","            with torch.no_grad():\n","                # Obtaining the logits from the model\n","                logits = net(seq, attn_masks, token_type_ids)\n","                labels = labels.unsqueeze(1) \n","\n","                # Computing loss\n","                loss = criterion(logits.squeeze(-1), labels.float())\n","                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n","\n","            # Backpropagating the gradients\n","            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n","            scaler.scale(loss).backward()\n","\n","            if (it + 1) % iters_to_accumulate == 0:\n","                # Optimization step\n","                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n","                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n","                # otherwise, opti.step() is skipped.\n","                scaler.step(opti)\n","                # Updates the scale for next iteration.\n","                scaler.update()\n","                # Adjust the learning rate based on the number of iterations.\n","                lr_scheduler.step()\n","                # Clear gradients\n","                opti.zero_grad()\n","\n","\n","            running_loss += loss.item()\n","\n","            if (it + 1) % print_every == 0:  # Print training loss information\n","                print()\n","                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n","                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n","\n","                running_loss = 0.0\n","\n","\n","        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n","        print()\n","        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n","\n","        if val_loss < best_loss:\n","            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n","            print()\n","            #net_copy = copy.deepcopy(net)  # save a copy of the model\n","            best_loss = val_loss\n","            best_ep = ep + 1\n","\n","    # Saving the model\n","    path_to_model='models/{}_lr_{}_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), best_ep)\n","    #torch.save(net_copy.state_dict(), path_to_model)\n","    print(\"The model has been saved in {}\".format(path_to_model))\n","\n","    del loss\n","    torch.cuda.empty_cache()"],"metadata":{"id":"8uZATGR_TUR8","executionInfo":{"status":"ok","timestamp":1686671743628,"user_tz":240,"elapsed":346,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":197,"outputs":[]},{"cell_type":"code","source":["bert_model = \"albert-base-v2\"  # 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2', 'bert-base-uncased', ...\n","freeze_bert = False  # if True, freeze the encoder weights and only update the classification layer weights\n","maxlen = 128  # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n","bs = 16  # batch size\n","iters_to_accumulate = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n","lr = 2e-5  # learning rate\n","epochs = 4  # number of training epochs"],"metadata":{"id":"f40Ky3NbTX8m","executionInfo":{"status":"ok","timestamp":1686668936488,"user_tz":240,"elapsed":2,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["df_train, df_val = train_test_split(df, test_size = 0.2, stratify = df['score_to_predict'], random_state=11)"],"metadata":{"id":"0quYzpLJTfEz","executionInfo":{"status":"ok","timestamp":1686670363230,"user_tz":240,"elapsed":1,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":165,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")  "],"metadata":{"id":"k98T2QjmV9Ky","executionInfo":{"status":"ok","timestamp":1686670365843,"user_tz":240,"elapsed":1857,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":166,"outputs":[]},{"cell_type":"code","source":["train_inputs = df_train.predict_from.values.tolist()\n","train_inputs\n","\n","# Tokenize the pair of sentences to get token ids, attention masks and token type ids\n","encoded_pair = tokenizer(train_inputs ,\n","                              padding=True,  # Pad to max_length\n","                              truncation=True,  # Truncate to max_length\n","                              return_tensors='pt')  # Return torch.Tensor objects\n","\n","train_token_ids = encoded_pair['input_ids']  # tensor of token ids\n","train_attn_masks = encoded_pair['attention_mask'] # binary tensor with \"0\" for padded values and \"1\" for the other values\n","train_token_type_ids = encoded_pair['token_type_ids']"],"metadata":{"id":"7ZIakjkgVtNs","executionInfo":{"status":"ok","timestamp":1686670367004,"user_tz":240,"elapsed":1166,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":167,"outputs":[]},{"cell_type":"code","source":["val_inputs = df_val.predict_from.values.tolist()\n","\n","\n","# Tokenize the pair of sentences to get token ids, attention masks and token type ids\n","encoded_pair_val = tokenizer(val_inputs ,\n","                              padding=True,  # Pad to max_length\n","                              truncation=True,  # Truncate to max_length\n","                              return_tensors='pt')  # Return torch.Tensor objects\n","\n","val_token_ids = encoded_pair_val['input_ids']  # tensor of token ids\n","val_attn_masks = encoded_pair_val['attention_mask']  # binary tensor with \"0\" for padded values and \"1\" for the other values\n","val_token_type_ids = encoded_pair_val['token_type_ids']"],"metadata":{"id":"-HiyNgXqWZh5","executionInfo":{"status":"ok","timestamp":1686670369226,"user_tz":240,"elapsed":3,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":168,"outputs":[]},{"cell_type":"code","source":["train_labels = df_train.score_to_predict.values -1\n","valid_labels = df_val.score_to_predict.values-1"],"metadata":{"id":"X_GnzhELWzPN","executionInfo":{"status":"ok","timestamp":1686670369864,"user_tz":240,"elapsed":3,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":169,"outputs":[]},{"cell_type":"code","source":["train_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwTGgiiqXbeO","executionInfo":{"status":"ok","timestamp":1686670003666,"user_tz":240,"elapsed":329,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"c927af70-eec0-48d3-accd-661141e90120"},"execution_count":143,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0,  ..., 0, 2, 0])"]},"metadata":{},"execution_count":143}]},{"cell_type":"code","source":["train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(valid_labels)  # Add a dimension to match the other tensors\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_token_ids, train_attn_masks, train_token_type_ids, train_labels)\n","train_dataloader = DataLoader(train_data, shuffle=True, batch_size=32)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(val_token_ids, val_attn_masks, val_token_type_ids, validation_labels)\n","validation_dataloader = DataLoader(validation_data, shuffle=False, batch_size=32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VqeU6OcxWngb","executionInfo":{"status":"ok","timestamp":1686670848227,"user_tz":240,"elapsed":362,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"5fa58c43-b5e8-49ce-c9ac-79ea7c031ded"},"execution_count":178,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-178-19d9c3f88fa6>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_labels = torch.tensor(train_labels)\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aq93ZPQsXoYR","executionInfo":{"status":"ok","timestamp":1686670057658,"user_tz":240,"elapsed":381,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"0ceea5ca-e795-4e67-e9ee-8af4c64d9cd4"},"execution_count":144,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7f55e05ec310>"]},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["#  Set all seeds to make reproducible results\n","set_seed(1)\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n","\n","if torch.cuda.device_count() > 1:  # if multiple GPUs\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    net = nn.DataParallel(net)\n","\n","net.to(device)\n","\n","criterion = nn.BCEWithLogitsLoss()\n","opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n","num_warmup_steps = 0 # The number of steps for the warmup phase.\n","num_training_steps = epochs * len(train_dataloader)  # The total number of training steps\n","t_total = (len(train_dataloader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n","lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n","\n","train_bert(net, criterion, opti, lr, lr_scheduler, train_dataloader, validation_dataloader, epochs, iters_to_accumulate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":712},"id":"1Ua8CsqeTdK6","executionInfo":{"status":"error","timestamp":1686671870036,"user_tz":240,"elapsed":2881,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"52df5026-a02d-439b-aa1b-28e64d5a89c5"},"execution_count":200,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","  0%|          | 0/407 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([32])\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/407 [00:00<?, ?it/s]\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 21>\u001b[0m:\u001b[94m21\u001b[0m                                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92mtrain_bert\u001b[0m:\u001b[94m32\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mloss.py\u001b[0m:\u001b[94m720\u001b[0m in \u001b[92mforward\u001b[0m                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 717 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.pos_weight: Optional[Tensor]                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 718 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 719 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor, target: Tensor) -> Tensor:                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 720 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m F.binary_cross_entropy_with_logits(\u001b[96minput\u001b[0m, target,                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 721 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │     \u001b[0m\u001b[96mself\u001b[0m.weight,                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 722 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │     \u001b[0mpos_weight=\u001b[96mself\u001b[0m.pos_weight,             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 723 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │     \u001b[0mreduction=\u001b[96mself\u001b[0m.reduction)               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/\u001b[0m\u001b[1;33mfunctional.py\u001b[0m:\u001b[94m3163\u001b[0m in                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92mbinary_cross_entropy_with_logits\u001b[0m                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3160 \u001b[0m\u001b[2m│   │   \u001b[0mreduction_enum = _Reduction.get_enum(reduction)                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3161 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3162 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (target.size() == \u001b[96minput\u001b[0m.size()):                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3163 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mTarget size (\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m) must be the same as input size (\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m)\u001b[0m\u001b[33m\"\u001b[0m.format(t  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3164 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3165 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m torch.binary_cross_entropy_with_logits(\u001b[96minput\u001b[0m, target, weight, pos_weight, red  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3166 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mValueError: \u001b[0mTarget size \u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m must be the same as input size \u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m203\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 21&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">21</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_bert</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">32</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loss.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">720</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 717 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.pos_weight: Optional[Tensor]                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 718 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 719 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor, target: Tensor) -&gt; Tensor:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 720 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.binary_cross_entropy_with_logits(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, target,                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 721 │   │   │   │   │   │   │   │   │   │   │   │     </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight,                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 722 │   │   │   │   │   │   │   │   │   │   │   │     </span>pos_weight=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.pos_weight,             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 723 │   │   │   │   │   │   │   │   │   │   │   │     </span>reduction=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.reduction)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">functional.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3163</span> in                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">binary_cross_entropy_with_logits</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3160 │   │   </span>reduction_enum = _Reduction.get_enum(reduction)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3161 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3162 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (target.size() == <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>.size()):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3163 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Target size ({}) must be the same as input size ({})\"</span>.format(t  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3164 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3165 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.binary_cross_entropy_with_logits(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, target, weight, pos_weight, red  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3166 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Target size <span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]))</span> must be the same as input size <span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">203</span><span style=\"font-weight: bold\">]))</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["results = {}\n","for i, df in enumerate(dfs, start = 6):\n","    name = unique_accessions[i]\n","    df = dfs[name]\n","    df['input'] = df['input'].apply(extract_first_string)\n","    score = train_model(df)\n","    results[name] = [score]  # Store score as a list\n","\n","    print(\"Koehn Kappa. = \" + str(score))\n","\n","# Create a DataFrame from the results\n","results_df = pd.DataFrame(results)\n","\n","# Save the DataFrame to a CSV file\n","results_df.to_csv('/content/drive/MyDrive/NAEP_Comp/Diberta/Cohen_Kappa' + str(name) + '.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a9a2476554dd47199058b0147200cfe8","9c030808a4fa48acb1099209868e563b","cd40c8b1961544dc9ce13eab6ee606f5","174cebdc42a042c7966c136fc6f36c63","a823fa91231648fd929cc1caddfcbbfe","7282d116b50b40e2a6846d47d9b77bfd","d8c31e61d8844da0af879935b523ea3b","f3a92841589f48788999bbfaed00dcd5","81754ed68ce840c880b08226c0b96a15","abeebc9bd20c49af86b2c820c9034765","a3ea5d4aacaa4a69b72e6248d0fd4995","ccddbd5e29734d19bdd857cdb3028db3","c45a56cce3b44efd960bc13bd016d610","569fd43456fc4accb812b603fb3e1f6a","60fefd6261524a7899a848a2b9f8d2df","f86c7b6120e2407bb83bf8e67ba4e8ec","f9373dddf3ac461a964dc0ba51c4968a","92ad14dbe43f4b7480cf16275effaea2","0b2b7615d7064bbba8e24cc4a6caf4e7","8c956a9245034aebb14b6bc93436146e","e8de782c1ca24861ae47b6c20cd2060a","e2b96fbb19ce4aceb9b67e980cce5818","d01b811d11394ed8acaa9f9a22b0f47b","e6af3b7666ac4f9495a14320d1fb3bd3","f5061a494e41483c9d63a243159d085d","85da2d6077f642a8a8a8e9971ef715dd","0a352874d0db4010b63ff4dce99aceef","3b8ddf2fe5dd4b9a99242a128333b8ef","707784fc22884fb592200b1b19cf0e5f","bca73c42b9df46d090f7423e080cc17f","5ce431c72fba4bb4a54cc973024e60b4","cfe92597489a4f89bd92d47d1a4acdd9","bd75aca71d2e4d69b22ac3694b2249db","84bfaf7992794928b73ae4f3886d927d","a36dd243238441f18500117526990b8c","018c177581204903bd733390ac4a311b","6a683f3b415941e3a6d4380a5fb13948","151b0701116a453e96212bcf671b8299","29678547842c49bb87952e648c7b9cbf","7055aef053874b7fa903b5b4e06a5637","585599d88bd84338bf69fc6d3f037e9e","5b91c44715574a2a8e46b8116d069237","39a6fa8a3bf94f00b487696ef04882a9","08391bc5f23f45f59831dfb449344a8b"]},"id":"cbOR97zM2xA2","outputId":"1acfee42-394c-4ebf-f317-eaf72c3f33b8","executionInfo":{"status":"error","timestamp":1685771058486,"user_tz":240,"elapsed":4045685,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/35874 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9a2476554dd47199058b0147200cfe8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='8970' max='8970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8970/8970 28:54, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Quadratic Cohen Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.472800</td>\n","      <td>0.471525</td>\n","      <td>0.732441</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.428500</td>\n","      <td>0.436142</td>\n","      <td>0.767565</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.414400</td>\n","      <td>0.436953</td>\n","      <td>0.772091</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.400600</td>\n","      <td>0.433888</td>\n","      <td>0.766775</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.382700</td>\n","      <td>0.432750</td>\n","      <td>0.774364</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Koehn Kappa. = 0.7743643673784506\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/19555 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccddbd5e29734d19bdd857cdb3028db3"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4890' max='4890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4890/4890 15:09, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Quadratic Cohen Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.022900</td>\n","      <td>0.855908</td>\n","      <td>0.371522</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.857600</td>\n","      <td>0.845780</td>\n","      <td>0.367913</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.829700</td>\n","      <td>0.834033</td>\n","      <td>0.387904</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.813400</td>\n","      <td>0.833402</td>\n","      <td>0.384171</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.809600</td>\n","      <td>0.831821</td>\n","      <td>0.380425</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Koehn Kappa. = 0.38042454991796826\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/12135 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01b811d11394ed8acaa9f9a22b0f47b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3035' max='3035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3035/3035 09:30, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Quadratic Cohen Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.841300</td>\n","      <td>0.654464</td>\n","      <td>0.827634</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.622800</td>\n","      <td>0.591269</td>\n","      <td>0.856429</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.571700</td>\n","      <td>0.594033</td>\n","      <td>0.871118</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.539000</td>\n","      <td>0.589501</td>\n","      <td>0.877014</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.520300</td>\n","      <td>0.583681</td>\n","      <td>0.877010</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Koehn Kappa. = 0.8770097271735666\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/15691 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84bfaf7992794928b73ae4f3886d927d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3925' max='3925' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3925/3925 12:54, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Quadratic Cohen Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.762400</td>\n","      <td>0.602932</td>\n","      <td>0.530187</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.584400</td>\n","      <td>0.577074</td>\n","      <td>0.607105</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.560200</td>\n","      <td>0.574085</td>\n","      <td>0.610302</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.524300</td>\n","      <td>0.570469</td>\n","      <td>0.609872</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.530800</td>\n","      <td>0.566220</td>\n","      <td>0.619914</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Koehn Kappa. = 0.6199138039522183\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 2>\u001b[0m:\u001b[94m3\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mIndexError: \u001b[0mlist index out of range\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 2&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>list index out of range\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["results"],"metadata":{"id":"0rosfesT0gI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q git+https://github.com/gmihaila/ml_things.git"],"metadata":{"id":"AVJNWg_BCJX9","executionInfo":{"status":"ok","timestamp":1686627119829,"user_tz":240,"elapsed":12985,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"143ba6a9-78ab-45ec-d119-86f2b6a4f1b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for ml-things (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["model_name_or_path = 'gpt2'"],"metadata":{"id":"_zs3J74hzz23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Gpt2ClassificationCollator(object):\n","\n","\n","\n","    def __init__(self, use_tokenizer, max_sequence_len=None):\n","\n","        # Tokenizer to be used inside the class.\n","        self.use_tokenizer = use_tokenizer\n","        # Check max sequence length.\n","        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n","\n","\n","        return\n","\n","    def __call__(self, sequences):\n","        print(sequences)\n","\n","\n","        # Get all texts from sequences list.\n","        texts = [sequence['text'] for sequence in sequences]\n","        # Get all labels from sequences list.\n","        labels = [sequence['label'] for sequence in sequences]\n","        # Encode all labels using label encoder.\n","        \n","        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n","        # Update the inputs with the associated encoded labels as tensor.\n","        inputs.update({'labels':torch.tensor(labels)})\n","\n","        return inputs\n"],"metadata":{"id":"RmGGDF66z4FU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(dataloader, optimizer_, scheduler_, device_):\n"," \n","\n","  # Use global variable for model.\n","  global model\n","\n","  # Tracking variables.\n","  predictions_labels = []\n","  true_labels = []\n","  # Total loss for this epoch.\n","  total_loss = 0\n","\n","  # Put the model into training mode.\n","  model.train()\n","\n","  # For each batch of training data...\n","  for batch in tqdm(dataloader, total=len(dataloader)):\n","\n","    # Add original labels - use later for evaluation.\n","    true_labels += batch['labels'].numpy().flatten().tolist()\n","    \n","    # move batch to device\n","    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n","    \n","    # Always clear any previously calculated gradients before performing a\n","    # backward pass.\n","    model.zero_grad()\n","\n","    outputs = model(**batch)\n","\n","\n","    loss, logits = outputs[:2]\n","\n","\n","    total_loss += loss.item()\n","\n","    # Perform a backward pass to calculate the gradients.\n","    loss.backward()\n","\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","\n","    optimizer_.step()\n","\n","    # Update the learning rate.\n","    scheduler_.step()\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","\n","    # Convert these logits to list of predicted labels values.\n","    predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n","\n","  # Calculate the average loss over the training data.\n","  avg_epoch_loss = total_loss / len(dataloader)\n","  \n","  # Return all true labels and prediction for future evaluations.\n","  return true_labels, predictions_labels, avg_epoch_loss"],"metadata":{"id":"rK-m_5F80EG7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validation(dataloader, device_):\n","\n","  # Use global variable for model.\n","  global model\n","\n","  # Tracking variables\n","  predictions_labels = []\n","  true_labels = []\n","  #total loss for this epoch.\n","  total_loss = 0\n","\n","\n","  model.eval()\n","\n","  # Evaluate data for one epoch\n","  for batch in tqdm(dataloader, total=len(dataloader)):\n","\n","    # add original labels\n","    true_labels += batch['labels'].numpy().flatten().tolist()\n","\n","    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n","\n","\n","    with torch.no_grad():        \n","\n","       \n","        outputs = model(**batch)\n","\n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple along with the logits. We will use logits\n","        # later to to calculate training accuracy.\n","        loss, logits = outputs[:2]\n","        \n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","\n","        total_loss += loss.item()\n","        \n","        # get predicitons to list\n","        predict_content = logits.argmax(axis=-1).flatten().tolist()\n","\n","        # update list\n","        predictions_labels += predict_content\n","\n","  # Calculate the average loss over the training data.\n","  avg_epoch_loss = total_loss / len(dataloader)\n","  score = cohen_kappa_score(predictions_labels,true_labels, weights = 'quadratic' )\n","  print()\n","  print(\"Cohen Kappa = \" + str(score))\n","\n","  # Return all true labels and prediciton for future evaluations.\n","  return true_labels, predictions_labels, avg_epoch_loss"],"metadata":{"id":"2cui2Qc20Yj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import (set_seed,\n","                          TrainingArguments,\n","                          Trainer,\n","                          GPT2Config,\n","                          GPT2Tokenizer,\n","                          AdamW, \n","                          get_linear_schedule_with_warmup,\n","                          GPT2ForSequenceClassification)\n","\n","# Set seed for reproducibility.\n","set_seed(123)\n","\n","# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n","epochs = 4\n","\n","# Number of batches - depending on the max sequence length and GPU memory.\n","# For 512 sequence length batch of 10 works without cuda memory issues.\n","# For small sequence length can try batch of 32 or higher.\n","batch_size = 32\n","\n","# Pad or truncate text sequences to a specific length\n","# if `None` it will use maximum sequence of word piece tokens allowed by model.\n","max_length = 60"],"metadata":{"id":"dgia9qF1006I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get model configuration.\n","print('Loading configuraiton...')\n","model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=df['score_to_predict'].nunique() + 1)\n","\n","# Get model's tokenizer.\n","print('Loading tokenizer...')\n","tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n","# default to left padding\n","tokenizer.padding_side = \"left\"\n","# Define PAD Token = EOS Token = 50256\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","\n","# Get the actual model.\n","print('Loading model...')\n","model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n","\n","# resize model embedding to match new tokenizer\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# fix model padding token id\n","model.config.pad_token_id = model.config.eos_token_id\n","\n","# Load model to defined device.\n","model.to(device)\n","print('Model loaded to `%s`'%device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"opa9jKKh0zil","executionInfo":{"status":"ok","timestamp":1686628214619,"user_tz":240,"elapsed":1850,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"80388d65-56e9-4536-9272-ee950caf9cff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading configuraiton...\n","Loading tokenizer...\n","Loading model...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded to `cuda`\n"]}]},{"cell_type":"code","source":["gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n","                                                          \n","                                                          max_sequence_len=max_length)"],"metadata":{"id":"hw3jbb1700ER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MathDataset(Dataset):\n","\n","  def __init__(self, df, use_tokenizer):\n","\n","    # Check if path exists.\n","    if not os.path.isdir(path):\n","      # Raise error if path is invalid.\n","      raise ValueError('Invalid `path` variable! Needs to be a directory')\n","    self.texts = df.predict_from.values\n","    self.labels = df.score_to_predict.values\n","    # Since the labels are defined by folders with data we loop \n","    # through each label.\n","    \n","\n","    # Number of exmaples.\n","    self.n_examples = len(self.labels)\n","    \n","\n","    return\n","\n","  def __len__(self):\n","\n","    return self.n_examples\n","\n","  def __getitem__(self, item):\n","\n","    return {'text':self.texts[item],\n","            'label':self.labels[item]}\n"],"metadata":{"id":"yGGbH6LG1RTV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = MathDataset(df, use_tokenizer=tokenizer)\n","print('Created `train_dataset` with %d examples!'%len(train_dataset))\n","\n","# Move pytorch dataset into dataloader.\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n","print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n","\n","print()\n","\n","print('Dealing with Validation...')\n","# Create pytorch dataset.\n","valid_dataset =  MathDataset(df, use_tokenizer=tokenizer)\n","print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n","\n","# Move pytorch dataset into dataloader.\n","valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n","print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CLXcKXNQ2tHY","executionInfo":{"status":"ok","timestamp":1686628414573,"user_tz":240,"elapsed":874,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"88b41eee-e452-47fb-f1c2-e1fffd7cbb1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Created `train_dataset` with 16275 examples!\n","Created `train_dataloader` with 509 batches!\n","\n","Dealing with Validation...\n","Created `valid_dataset` with 16275 examples!\n","Created `eval_dataloader` with 509 batches!\n"]}]},{"cell_type":"code","source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # default is 1e-8.\n","                  )\n","\n","# Total number of training steps is number of batches * number of epochs.\n","# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n","# us the number of batches.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","# Store the average loss after each epoch so we can plot them.\n","all_loss = {'train_loss':[], 'val_loss':[]}\n","all_acc = {'train_acc':[], 'val_acc':[]}\n","\n","# Loop through each epoch.\n","print('Epoch')\n","for epoch in tqdm(range(epochs)):\n","  print()\n","  print('Training on batches...')\n","  # Perform one full pass over the training set.\n","  train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n","  train_acc = accuracy_score(train_labels, train_predict)\n","\n","  # Get prediction form model on validation data. \n","  print('Validation on batches...')\n","  valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n","  val_acc = accuracy_score(valid_labels, valid_predict)\n","\n","  # Print loss and accuracy values to see how training evolves.\n","  print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n","  print()\n","\n","  # Store the loss value for plotting the learning curve.\n","  all_loss['train_loss'].append(train_loss)\n","  all_loss['val_loss'].append(val_loss)\n","  all_acc['train_acc'].append(train_acc)\n","  all_acc['val_acc'].append(val_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1eNtnNMy_howZ20QrjScAE6hARQl-YT3h"},"id":"Lvng-OSm2uLc","executionInfo":{"status":"error","timestamp":1686629517267,"user_tz":240,"elapsed":1084668,"user":{"displayName":"Chandramani","userId":"15720194962338037264"}},"outputId":"e40131c3-77ab-403f-ced0-9f8d07856c82"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"UTnkf73q3IWj"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a9a2476554dd47199058b0147200cfe8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9c030808a4fa48acb1099209868e563b","IPY_MODEL_cd40c8b1961544dc9ce13eab6ee606f5","IPY_MODEL_174cebdc42a042c7966c136fc6f36c63"],"layout":"IPY_MODEL_a823fa91231648fd929cc1caddfcbbfe"}},"9c030808a4fa48acb1099209868e563b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7282d116b50b40e2a6846d47d9b77bfd","placeholder":"​","style":"IPY_MODEL_d8c31e61d8844da0af879935b523ea3b","value":"Map: 100%"}},"cd40c8b1961544dc9ce13eab6ee606f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3a92841589f48788999bbfaed00dcd5","max":35874,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81754ed68ce840c880b08226c0b96a15","value":35874}},"174cebdc42a042c7966c136fc6f36c63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abeebc9bd20c49af86b2c820c9034765","placeholder":"​","style":"IPY_MODEL_a3ea5d4aacaa4a69b72e6248d0fd4995","value":" 35874/35874 [00:04&lt;00:00, 9716.49 examples/s]"}},"a823fa91231648fd929cc1caddfcbbfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"7282d116b50b40e2a6846d47d9b77bfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8c31e61d8844da0af879935b523ea3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3a92841589f48788999bbfaed00dcd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81754ed68ce840c880b08226c0b96a15":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"abeebc9bd20c49af86b2c820c9034765":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3ea5d4aacaa4a69b72e6248d0fd4995":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccddbd5e29734d19bdd857cdb3028db3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c45a56cce3b44efd960bc13bd016d610","IPY_MODEL_569fd43456fc4accb812b603fb3e1f6a","IPY_MODEL_60fefd6261524a7899a848a2b9f8d2df"],"layout":"IPY_MODEL_f86c7b6120e2407bb83bf8e67ba4e8ec"}},"c45a56cce3b44efd960bc13bd016d610":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9373dddf3ac461a964dc0ba51c4968a","placeholder":"​","style":"IPY_MODEL_92ad14dbe43f4b7480cf16275effaea2","value":"Map:  97%"}},"569fd43456fc4accb812b603fb3e1f6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b2b7615d7064bbba8e24cc4a6caf4e7","max":19555,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c956a9245034aebb14b6bc93436146e","value":19555}},"60fefd6261524a7899a848a2b9f8d2df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8de782c1ca24861ae47b6c20cd2060a","placeholder":"​","style":"IPY_MODEL_e2b96fbb19ce4aceb9b67e980cce5818","value":" 19000/19555 [00:02&lt;00:00, 9141.95 examples/s]"}},"f86c7b6120e2407bb83bf8e67ba4e8ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"f9373dddf3ac461a964dc0ba51c4968a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92ad14dbe43f4b7480cf16275effaea2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b2b7615d7064bbba8e24cc4a6caf4e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c956a9245034aebb14b6bc93436146e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8de782c1ca24861ae47b6c20cd2060a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2b96fbb19ce4aceb9b67e980cce5818":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d01b811d11394ed8acaa9f9a22b0f47b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6af3b7666ac4f9495a14320d1fb3bd3","IPY_MODEL_f5061a494e41483c9d63a243159d085d","IPY_MODEL_85da2d6077f642a8a8a8e9971ef715dd"],"layout":"IPY_MODEL_0a352874d0db4010b63ff4dce99aceef"}},"e6af3b7666ac4f9495a14320d1fb3bd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b8ddf2fe5dd4b9a99242a128333b8ef","placeholder":"​","style":"IPY_MODEL_707784fc22884fb592200b1b19cf0e5f","value":"Map: 100%"}},"f5061a494e41483c9d63a243159d085d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bca73c42b9df46d090f7423e080cc17f","max":12135,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ce431c72fba4bb4a54cc973024e60b4","value":12135}},"85da2d6077f642a8a8a8e9971ef715dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfe92597489a4f89bd92d47d1a4acdd9","placeholder":"​","style":"IPY_MODEL_bd75aca71d2e4d69b22ac3694b2249db","value":" 12135/12135 [00:01&lt;00:00, 8267.64 examples/s]"}},"0a352874d0db4010b63ff4dce99aceef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"3b8ddf2fe5dd4b9a99242a128333b8ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"707784fc22884fb592200b1b19cf0e5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bca73c42b9df46d090f7423e080cc17f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ce431c72fba4bb4a54cc973024e60b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfe92597489a4f89bd92d47d1a4acdd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd75aca71d2e4d69b22ac3694b2249db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84bfaf7992794928b73ae4f3886d927d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a36dd243238441f18500117526990b8c","IPY_MODEL_018c177581204903bd733390ac4a311b","IPY_MODEL_6a683f3b415941e3a6d4380a5fb13948"],"layout":"IPY_MODEL_151b0701116a453e96212bcf671b8299"}},"a36dd243238441f18500117526990b8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29678547842c49bb87952e648c7b9cbf","placeholder":"​","style":"IPY_MODEL_7055aef053874b7fa903b5b4e06a5637","value":"Map:  96%"}},"018c177581204903bd733390ac4a311b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_585599d88bd84338bf69fc6d3f037e9e","max":15691,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b91c44715574a2a8e46b8116d069237","value":15691}},"6a683f3b415941e3a6d4380a5fb13948":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39a6fa8a3bf94f00b487696ef04882a9","placeholder":"​","style":"IPY_MODEL_08391bc5f23f45f59831dfb449344a8b","value":" 15000/15691 [00:02&lt;00:00, 9290.41 examples/s]"}},"151b0701116a453e96212bcf671b8299":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"29678547842c49bb87952e648c7b9cbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7055aef053874b7fa903b5b4e06a5637":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"585599d88bd84338bf69fc6d3f037e9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b91c44715574a2a8e46b8116d069237":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39a6fa8a3bf94f00b487696ef04882a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08391bc5f23f45f59831dfb449344a8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74ac38569a654e32b65929a298c7ff80":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e4976109cfb478899fe7aa050d4f5b4","IPY_MODEL_76288f40d54b4b798cf336e852777640","IPY_MODEL_b58e66fde712460dadaa883edfc92e72"],"layout":"IPY_MODEL_7f3da735e26d4e66873ec9bdfdec685a"}},"8e4976109cfb478899fe7aa050d4f5b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5818970356c4b82a1618309acf9adc5","placeholder":"​","style":"IPY_MODEL_3e652a690e2249708f53f16f2c4144cb","value":"Downloading (…)lve/main/config.json: 100%"}},"76288f40d54b4b798cf336e852777640":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7da202e520f6415baa18d2e6825bba36","max":684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d0f01238c5b4b2bbadc0e05022585d3","value":684}},"b58e66fde712460dadaa883edfc92e72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cedc48a67da44b0a716fd8ecfcf5726","placeholder":"​","style":"IPY_MODEL_b5bef2680da54e38bae557a42035fdf3","value":" 684/684 [00:00&lt;00:00, 59.0kB/s]"}},"7f3da735e26d4e66873ec9bdfdec685a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5818970356c4b82a1618309acf9adc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e652a690e2249708f53f16f2c4144cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7da202e520f6415baa18d2e6825bba36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d0f01238c5b4b2bbadc0e05022585d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7cedc48a67da44b0a716fd8ecfcf5726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5bef2680da54e38bae557a42035fdf3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e88cb1558dc247288450bd0d4eea1ec3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c194bafa7b048d49c54806122d7d3f2","IPY_MODEL_8b79f56950954b9dbb013b96f12269ef","IPY_MODEL_189052d0d1864135a14338387da77ca3"],"layout":"IPY_MODEL_85e7a4ada5bf41a9a9b9cf5bbc81bf64"}},"4c194bafa7b048d49c54806122d7d3f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eafc184aa1049878cd3aabfb9174264","placeholder":"​","style":"IPY_MODEL_ec301288254648a5bdf372360310f35c","value":"Downloading (…)ve/main/spiece.model: 100%"}},"8b79f56950954b9dbb013b96f12269ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5af8b14ba964fc9b4196e4c3a4bdd53","max":760289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55631421e2164a008b27dd23e8c16c54","value":760289}},"189052d0d1864135a14338387da77ca3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fbd6549ddeb42d78b6676f374a710c9","placeholder":"​","style":"IPY_MODEL_c575197f1154494ea58d62272f24ee5d","value":" 760k/760k [00:00&lt;00:00, 773kB/s]"}},"85e7a4ada5bf41a9a9b9cf5bbc81bf64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eafc184aa1049878cd3aabfb9174264":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec301288254648a5bdf372360310f35c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5af8b14ba964fc9b4196e4c3a4bdd53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55631421e2164a008b27dd23e8c16c54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fbd6549ddeb42d78b6676f374a710c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c575197f1154494ea58d62272f24ee5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ca49499252e40868e4341b821639635":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d5ddf51e3fec41b5975318d1eb1e0d1e","IPY_MODEL_8dd2e31727b243a0a7f234ce59f6bd21","IPY_MODEL_0a83024af29b4d00a71c4f3ebf390163"],"layout":"IPY_MODEL_f86aa85681d84a88b98358b279c0266d"}},"d5ddf51e3fec41b5975318d1eb1e0d1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93b343a563fa4744abffeb10f418f16e","placeholder":"​","style":"IPY_MODEL_ca49bc7b633a4ebf87d6694ae10e7a83","value":"Downloading (…)/main/tokenizer.json: 100%"}},"8dd2e31727b243a0a7f234ce59f6bd21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fabf410b4904b3685b384a360d7b360","max":1312669,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad7a95b794ee4d4da65638d46f661857","value":1312669}},"0a83024af29b4d00a71c4f3ebf390163":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c86dc935add94bb79c1a158005eabc2f","placeholder":"​","style":"IPY_MODEL_25bf8d89478949b1b20bd5d178bcab81","value":" 1.31M/1.31M [00:00&lt;00:00, 5.28MB/s]"}},"f86aa85681d84a88b98358b279c0266d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93b343a563fa4744abffeb10f418f16e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca49bc7b633a4ebf87d6694ae10e7a83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fabf410b4904b3685b384a360d7b360":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad7a95b794ee4d4da65638d46f661857":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c86dc935add94bb79c1a158005eabc2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25bf8d89478949b1b20bd5d178bcab81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d8a0967fc3f45aeaf8d1938fdad1aee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a25b0b6814b84a2393aad4eb68ca4028","IPY_MODEL_6a233144008e4f14b89f1beb8a726376","IPY_MODEL_8389bcc4ba92416f8d6e9ff480a77565"],"layout":"IPY_MODEL_85684b4207c14818a61af5e99091566e"}},"a25b0b6814b84a2393aad4eb68ca4028":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abfc5599cede495485882192b05ca1f1","placeholder":"​","style":"IPY_MODEL_f0ffdc8f889a4fc8a1f6340905a28bc7","value":"Map:  98%"}},"6a233144008e4f14b89f1beb8a726376":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1054c398bc149ed856303080c779ac1","max":16275,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76251b2f815745ef96cf5cfdb54e6247","value":16275}},"8389bcc4ba92416f8d6e9ff480a77565":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a37ae52ddf4642ec86c633bb1eac26ef","placeholder":"​","style":"IPY_MODEL_7ecd8ea34fe24a2993f8ceb1050b1d3e","value":" 16000/16275 [00:01&lt;00:00, 7647.90 examples/s]"}},"85684b4207c14818a61af5e99091566e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"abfc5599cede495485882192b05ca1f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0ffdc8f889a4fc8a1f6340905a28bc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1054c398bc149ed856303080c779ac1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76251b2f815745ef96cf5cfdb54e6247":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a37ae52ddf4642ec86c633bb1eac26ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ecd8ea34fe24a2993f8ceb1050b1d3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c4b207d9743483c9167e43d75ec3cfa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aaf1c666ea1c4380bc13b7bcac7dbd07","IPY_MODEL_ccfa7def232e40c6a18194291e13f9f0","IPY_MODEL_df9c0d4ba068499793e79e5c0c344ad6"],"layout":"IPY_MODEL_b5ef2cc9035646a689de2c26757a302b"}},"aaf1c666ea1c4380bc13b7bcac7dbd07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea5f6745f3464abd9aa0976496f05803","placeholder":"​","style":"IPY_MODEL_172313a2bf7248908c3ad2e38d746be7","value":"Downloading pytorch_model.bin: 100%"}},"ccfa7def232e40c6a18194291e13f9f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_84081067b6f2433fbcc909b59d9a14e2","max":47376696,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3479b2ea85e4120858372f48097d3ea","value":47376696}},"df9c0d4ba068499793e79e5c0c344ad6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4b1bb96a4f441b482343654e681f901","placeholder":"​","style":"IPY_MODEL_a9e4d8d0c6b44f3c9c8fc8c2572689ce","value":" 47.4M/47.4M [00:00&lt;00:00, 161MB/s]"}},"b5ef2cc9035646a689de2c26757a302b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea5f6745f3464abd9aa0976496f05803":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"172313a2bf7248908c3ad2e38d746be7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84081067b6f2433fbcc909b59d9a14e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3479b2ea85e4120858372f48097d3ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f4b1bb96a4f441b482343654e681f901":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9e4d8d0c6b44f3c9c8fc8c2572689ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9141c1c72e494250823c4a0713acd3c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e954b0a47bb4cfa9bf86d42a80cab2c","IPY_MODEL_3161252546e04f22a9b6a3fde2807c52","IPY_MODEL_0d5c857289ba4e7eae167cb924fd245f"],"layout":"IPY_MODEL_a6e9ae3ca0f94ace8a0c9ebce1b7e553"}},"6e954b0a47bb4cfa9bf86d42a80cab2c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1cce93214174ed798af8a3d3ce4b315","placeholder":"​","style":"IPY_MODEL_7cbb58badf1f406181641bac69dc935c","value":"Map: 100%"}},"3161252546e04f22a9b6a3fde2807c52":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_56293ee565f24ed090571d515e4d94a9","max":2018,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a21c0ddc9af4871ae65fd10cdd5fe84","value":2018}},"0d5c857289ba4e7eae167cb924fd245f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3075e14f3194feda746bc256342a7cb","placeholder":"​","style":"IPY_MODEL_0ca3fc84c0494c6884e7c8c7dd0b0f7e","value":" 2018/2018 [00:00&lt;00:00, 16191.03 examples/s]"}},"a6e9ae3ca0f94ace8a0c9ebce1b7e553":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d1cce93214174ed798af8a3d3ce4b315":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cbb58badf1f406181641bac69dc935c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56293ee565f24ed090571d515e4d94a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a21c0ddc9af4871ae65fd10cdd5fe84":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3075e14f3194feda746bc256342a7cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ca3fc84c0494c6884e7c8c7dd0b0f7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}