{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1s5jQ-AEdRs7qGo8EZXRDrB1qncvMCdvF","authorship_tag":"ABX9TyOoM1BLgpVJzIpYprdGTTNb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers==4.28.0"],"metadata":{"id":"spFVLb2wHy6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets\n","!pip install -U git+https://github.com/huggingface/accelerate.git"],"metadata":{"id":"EvaCQ_XFY-XJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install nlpaug"],"metadata":{"id":"oLY-4xR-C0yp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sacremoses"],"metadata":{"id":"Eb54MdRSqinN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HsTAUz6oHpVb"},"outputs":[],"source":["# Import libraries\n","\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","import seaborn as sns\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import initializers\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFDistilBertModel, DistilBertConfig"]},{"cell_type":"code","source":["# List of unique accessions\n","unique_accessions = ['VH134067', 'VH139380', 'VH266015', 'VH266510', 'VH269384',\n","                     'VH271613', 'VH302907', 'VH304954', 'VH507804', 'VH525628']\n","\n","\n","\n","# Dictionary to store the dataframes\n","dfs = {}\n","\n","# Loop through the unique accessions\n","for accession in unique_accessions:\n","    # Create the dataframe name\n","    path = '/content/drive/MyDrive/NAEP_Comp/'\n","    df_name = 'df_' + accession\n","\n","    # Read the CSV file into a dataframe\n","    df = pd.read_csv(path + df_name + '.csv')\n","\n","    # Add the dataframe to the dictionary\n","    dfs[accession] = df"],"metadata":{"id":"kqnWEnZueNLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = dfs['VH525628']"],"metadata":{"id":"x5RJjF8WHxCr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.dropna(subset=['parsed_xml_v1'])"],"metadata":{"id":"i5ind1TwIV62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"45F34IU2Ijoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(df['predict_from_onestepall'], df['assigned_score'], test_size=0.2, stratify=df['assigned_score'], random_state=42)\n","X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, stratify = y_test, random_state=42)\n","X_test.shape, X_train.shape, X_valid.shape\n"],"metadata":{"id":"mlAwGkDLHp2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Our training data has   ', len(X_train.index), ' rows.')\n","print('Our validation data has ', len(X_valid.index), ' rows.')\n","print('Our test data has       ', len(X_test.index), ' rows.')"],"metadata":{"id":"HL-u6aW2JSpq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train -= 1\n","y_test -= 1\n","y_valid -= 1"],"metadata":{"id":"Nz9E1xqcLlVh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.unique(y_train\n","          )"],"metadata":{"id":"O4pH5tcwLv2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import transformers\n","import torch\n","import csv\n","\n","from datasets import Dataset,load_dataset, load_from_disk, load_metric\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import TrainingArguments, Trainer, AdamW\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader"],"metadata":{"id":"YXaTRmPUZCsP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Change model to pretrain here\n","MODEL = \"distilbert-base-uncased\""],"metadata":{"id":"lShaBxYjZC3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = dfs['VH139380']"],"metadata":{"id":"OZ7je2-Jeigv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Include columns that are important (features, labels, student_id)\n","df = df[[\"student_id\", \"predict_from\", \"score_to_predict\"]].set_index(\"student_id\").fillna(\"\")\n","df['labels'] = df['score_to_predict'] - 1\n","df.head()"],"metadata":{"id":"-7nS4etdZETU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to dataset format\n","dataset = Dataset.from_pandas(df, preserve_index=False)\n","dataset = dataset.train_test_split(test_size=0.1, seed=11)\n","dataset"],"metadata":{"id":"GvAkQYAiZHNh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create model and tokenizer\n","# Make sure the num_labels argument matches the question (it will usually be 2, for correct/incorrect)\n","# Some questions may require more than one model (for more than one written section)\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=3)\n","Path = '/content/VH139380_b_distilbert.pth'\n","model.load_state_dict(torch.load(Path))\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)"],"metadata":{"id":"KhAPYmGTZUsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize the data\n","model.resize_token_embeddings(len(tokenizer))\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"predict_from\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)"],"metadata":{"id":"LDBmdFzlZOwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()"],"metadata":{"id":"fMA8n7jfyKuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(output_dir=\"test_trainer\",\n","                                  logging_strategy=\"epoch\",\n","                                  evaluation_strategy=\"epoch\",\n","                                  per_device_train_batch_size=32,\n","                                  per_device_eval_batch_size=32,\n","                                  num_train_epochs=2,\n","                                  save_total_limit = 2,\n","                                  save_strategy = 'no',\n","                                  load_best_model_at_end=False\n","                                  )\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    compute_metrics=None,\n",")\n","pred, actual, _ = trainer.predict(tokenized_datasets['test'])"],"metadata":{"id":"lpiJqucyySGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_labels = np.argmax(pred, axis=1)\n"],"metadata":{"id":"rIs9rTcvy5wa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_labels"],"metadata":{"id":"-yq3gwLDy_6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cohen_kappa_score(actual, pred_labels, weights='quadratic')"],"metadata":{"id":"gybVideIzFdL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nlpaug.augmenter.word as naw\n","\n","def detect_minority_majority_classes(df, label_column):\n","\n","    class_counts = df[label_column].value_counts()\n","    minority_classes = class_counts[class_counts < class_counts.max()].index.tolist()\n","    majority_class = class_counts.idxmax()\n","    return minority_classes, majority_class\n","\n","def augment_minority_class_text(df, text_column, label_column):\n","    augmented_texts = []\n","    aug = naw.SynonymAug(aug_src='wordnet',aug_max=2)\n","    minority_classes, majority_class = detect_minority_majority_classes(df, label_column)\n","    print(df[label_column].value_counts())\n","    \n","    for minority_class in minority_classes:\n","        # Filter the dataframe to get only the minority class rows\n","        minority_df = df[df[label_column] == minority_class]\n","        majority_df = df[df[label_column] == majority_class]\n","        minority_count = len(minority_df)\n","        majority_count = len(majority_df)\n","        \n","        # Check if augmentation is required based on class imbalance\n","        if minority_count >= 0.6* majority_count:\n","            continue\n","\n","        # Calculate the number of augmentations required\n","        num_augmentations = int(0.6 * majority_count) - minority_count\n","        \n","        # Augment the text of the minority class\n","        while num_augmentations > 0:\n","            for text in minority_df[text_column]:\n","                augmented_text = aug.augment(text)\n","                if augmented_text:\n","                    augmented_texts.append((augmented_text[0], minority_class))  # Append augmented text with the minority class label\n","                    num_augmentations -= 1\n","                    if num_augmentations == 0:\n","                        break\n","\n","    # Create a new dataframe with augmented texts\n","    augmented_df = pd.DataFrame(augmented_texts, columns=[text_column, label_column])\n","    \n","    # Concatenate the augmented dataframe with the original dataframe\n","    augmented_df = pd.concat([df, augmented_df], ignore_index=True)\n","    print(augmented_df[label_column].value_counts())\n","    return augmented_df\n","\n"],"metadata":{"id":"sf5QM1X1CyfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"id":"4px8-fPyDZhA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import contractions"],"metadata":{"id":"Al5Oufw1Dhw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(eval_pred):\n","    logits, labels = eval_pred.predictions, eval_pred.label_ids\n","    preds = logits.argmax(axis=1)\n","    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n","    return {\"cohen_kappa\": kappa}"],"metadata":{"id":"SZSRHJlTDHP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import cohen_kappa_score\n","import os, sys, itertools, re"],"metadata":{"id":"A2TPtQ3_DSHI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text_blob\"], padding=\"max_length\", truncation=True)"],"metadata":{"id":"6BMuQ5xZFXiz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess(text):\n","    text=text.lower()\n","    # remove hyperlinks\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n","    #Replace &amp, &lt, &gt with &,<,> respectively\n","    text=text.replace(r'&amp;?',r'and')\n","    text=text.replace(r'&lt;',r'<')\n","    text=text.replace(r'&gt;',r'>')\n","    #remove hashtag sign\n","    text=re.sub(r\"#\",\"\",text)   \n","    #remove mentions\n","    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n","    #text=re.sub(r\"@\",\"\",text)\n","    #remove non ascii chars\n","    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n","    #remove some puncts (except . ! ?)\n","    text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\n","    text=re.sub(r'[!]+','!',text)\n","    text=re.sub(r'[?]+','?',text)\n","    text=re.sub(r'[.]+','.',text)\n","    text=re.sub(r\"'\",\"\",text)\n","    text=re.sub(r\"\\(\",\"\",text)\n","    text=re.sub(r\"\\)\",\"\",text)\n","    \n","    text=\" \".join(text.split())\n","    return text"],"metadata":{"id":"FYV4fy1oDBzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(train_dataset,eval_dataset, test_indexes, name, model) :\n","  # AdamW Training\n","  training_args = TrainingArguments(output_dir=\"test_trainer\",\n","                                    logging_strategy=\"epoch\",\n","                                    evaluation_strategy=\"epoch\",\n","                                    per_device_train_batch_size=32,\n","                                    per_device_eval_batch_size=32,\n","                                    num_train_epochs=5,\n","                                    save_total_limit = 2,\n","                                    save_strategy = 'epoch',\n","                                    load_best_model_at_end=True\n","                                    )\n","  trainer = Trainer(\n","      model=model,\n","      args=training_args,\n","      train_dataset=train_dataset,\n","      eval_dataset=eval_dataset,\n","      compute_metrics=compute_metrics,\n","  )\n","  trainer.train()\n","  pred, actual, _ = trainer.predict(eval_dataset)\n","  pred_labels = np.argmax(pred, axis=1)\n","  results_df = pd.DataFrame(index=test_indexes)\n","  results_df['indexes'] = test_indexes\n","  results_df['True Labels'] = actual + 1\n","  results_df['PredictedValue'] = pred_labels + 1\n","  results_df.to_csv('/content/drive/MyDrive/NAEP_Comp/DistillBert1/' + name + '.csv')\n","  kappa_score = cohen_kappa_score(actual, pred_labels, weights='quadratic')\n","  return kappa_score\n","\n","\n"],"metadata":{"id":"i_XHg-MhhahF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_main = pd.read_csv('/content/drive/MyDrive/NAEP_Comp/df_cleaned.csv')"],"metadata":{"id":"AMw-Q1nul5cO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_1 = df_main[df_main['accession'] == 'VH525628']"],"metadata":{"id":"5mEEMNMpl8fC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(df, name) :\n","  # Convert to dataset format\n","  # Include columns that are important (features, labels, student_id)\n","  df = df[[\"student_id\", \"text_blob\", \"score_to_predict\"]].fillna(\"\")\n","  df['text_blob'] = df['text_blob'].apply(preprocess)\n","  df['text_blob'] = df['text_blob'].fillna('N/A')\n","  df['text_blob'] = df['text_blob'].apply(contractions.fix)\n","  df['labels'] = df['score_to_predict'] - 1\n","  df_train, df_valid = train_test_split(df, test_size = 0.2, stratify = df['labels'], random_state=11 )\n","  df_balanced = augment_minority_class_text(df_train, 'text_blob','labels')\n","  dataset_train = Dataset.from_pandas(df_balanced, preserve_index=False)\n","  dataset_valid = Dataset.from_pandas(df_valid, preserve_index=False)\n","  test_indexes = dataset_valid['student_id']\n","  model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=df['labels'].nunique())\n","  tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","  model.resize_token_embeddings(len(tokenizer))\n","  train_tokenized =dataset_train.map(tokenize_function, batched=True)\n","  valid_tokenized = dataset_valid.map(tokenize_function, batched=True)\n","  \n","  score = train(train_tokenized,valid_tokenized, test_indexes, name, model=model)\n","\n","  return score\n","\n"],"metadata":{"id":"w4RuiPXIghQg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_model(df_1, 'test_distill1')"],"metadata":{"id":"WdydbTcbEtBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = {}\n","for i, df in enumerate(dfs):\n","    name = unique_accessions[i]\n","    df = dfs[name]\n","    print(name)\n","    score = train_model(df, name)\n","    results[name] = [score]  # Store score as a list\n","    print(score)\n","\n","# Create a DataFrame from the results\n","results_df = pd.DataFrame(results)\n","\n","# Save the DataFrame to a CSV file\n","results_df.to_csv('//content/drive/MyDrive/NAEP_Comp/DistillBert/Results_Cohen_' + str(name) + '.csv', index=False)"],"metadata":{"id":"SF8nZm3vi-Wf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# AdamW Training\n","training_args = TrainingArguments(output_dir=\"test_trainer\",\n","                                  logging_strategy=\"epoch\",\n","                                  evaluation_strategy=\"epoch\",\n","                                  per_device_train_batch_size=32,\n","                                  per_device_eval_batch_size=32,\n","                                  num_train_epochs=2,\n","                                  save_total_limit = 2,\n","                                  save_strategy = 'no',\n","                                  load_best_model_at_end=False\n","                                  )\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    compute_metrics=None,\n",")\n","trainer.train()"],"metadata":{"id":"_O0mv3KgZQ83"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"_TYq9O6BxBQT"}},{"cell_type":"code","source":["# Get predictions for parts graded via model\n","pred, actual, _ = trainer.predict(tokenized_datasets['test'])"],"metadata":{"id":"7y__w4T9Ze8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_labels = np.argmax(pred, axis=1)"],"metadata":{"id":"YT2Uc9dgb6fB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_labels"],"metadata":{"id":"g8NmIQMXcBmh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the QWK using predictions on test data\n","cohen_kappa_score(actual, pred_labels, weights='quadratic')"],"metadata":{"id":"nR1BCImVb1uT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save model\n","model_save_name = 'VH139380_b_distilbert.pth'\n","path = model_save_name\n","torch.save(model.state_dict(), path)"],"metadata":{"id":"Z4aYJbHBb3to"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9-7B_m5hcsVM"},"execution_count":null,"outputs":[]}]}