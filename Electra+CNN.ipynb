{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1ffGRaXlnYEQpWqRpJ-34ApeBFhQyZFo1","authorship_tag":"ABX9TyOOwDbmV22XQKN6tDsZlEzA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers==4.28.0\n","!pip install optuna"],"metadata":{"id":"sLDImosNLliN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjbEKVFkLaKb"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import get_linear_schedule_with_warmup, AdamW\n","from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n","import time, datetime, random, optuna, re, string\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from optuna.pruners import SuccessiveHalvingPruner\n","from optuna.samplers import TPESampler\n","from torch.cuda.amp import autocast, GradScaler\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","from transformers import BertModel, BertTokenizer\n","\n","SEED = 15\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"]},{"cell_type":"code","source":["device = torch.device(\"cuda\")"],"metadata":{"id":"C3hW-xGLLhKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List of unique accessions\n","unique_accessions = ['VH134067', 'VH139380', 'VH266015', 'VH266510', 'VH269384',\n","                     'VH271613', 'VH302907', 'VH304954', 'VH507804', 'VH525628']\n","\n","\n","\n","# Dictionary to store the dataframes\n","dfs = {}\n","\n","# Loop through the unique accessions\n","for accession in unique_accessions:\n","    # Create the dataframe name\n","    path = '/content/drive/MyDrive/NAEP_Comp/'\n","    df_name = 'df_cleaned' + accession\n","\n","    # Read the CSV file into a dataframe\n","    df = pd.read_csv(path + df_name + '.csv')\n","\n","    # Add the dataframe to the dictionary\n","    dfs[accession] = df"],"metadata":{"id":"1YevE_BiL4Sc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = dfs['VH525628']\n"],"metadata":{"id":"_a2zd1v9MEGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.dropna(subset = ['predict_from'])"],"metadata":{"id":"3NzuO3OjMl24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess(text):\n","    text=text.lower()\n","    # remove hyperlinks\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n","    #Replace &amp, &lt, &gt with &,<,> respectively\n","    text=text.replace(r'&amp;?',r'and')\n","    text=text.replace(r'&lt;',r'<')\n","    text=text.replace(r'&gt;',r'>')\n","    #remove hashtag sign\n","    text=re.sub(r\"#\",\"\",text)   \n","    #remove mentions\n","    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n","    #text=re.sub(r\"@\",\"\",text)\n","    #remove non ascii chars\n","    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n","    #remove some puncts (except . ! ?)\n","    text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\n","    text=re.sub(r'[!]+','!',text)\n","    text=re.sub(r'[?]+','?',text)\n","    text=re.sub(r'[.]+','.',text)\n","    text=re.sub(r\"'\",\"\",text)\n","    text=re.sub(r\"\\(\",\"\",text)\n","    text=re.sub(r\"\\)\",\"\",text)\n","    \n","    text=\" \".join(text.split())\n","    return text"],"metadata":{"id":"0Vys9rRrM3SR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['predict_from'] = df['predict_from'].apply(preprocess)"],"metadata":{"id":"Dlwnhl5tNKpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers.models.bert.modeling_bert import BertForSequenceClassification\n","# instantiate BERT model with hidden states\n","model = BertForSequenceClassification.from_pretrained(MODEL, output_hidden_states=True).cuda()"],"metadata":{"id":"UbHN7Mz9PYCD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import ElectraForSequenceClassification, ElectraTokenizer, AutoTokenizer, AutoModelForSequenceClassification"],"metadata":{"id":"HGHFpod9T0Kf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL = \"tbs17/MathBERT\""],"metadata":{"id":"2VjoFL0TQVp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(MODEL, do_lower_case=True)"],"metadata":{"id":"AhFRgCvxTylt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def geIdsAndMasks (df) : \n","    input_ids = []\n","    attention_masks = []\n","    for sentence in df['predict_from'].tolist():\n","        dictionary = tokenizer.encode_plus(\n","                            sentence,                      \n","                            add_special_tokens = True,\n","                            max_length = 80,\n","                            truncation=True,\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,\n","                            return_tensors = 'pt',\n","                    )\n","        # encode_plus returns a dictionary \n","        input_ids.append(dictionary['input_ids'])\n","        attention_masks.append(dictionary['attention_mask'])\n","\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    return input_ids, attention_masks"],"metadata":{"id":"wfuo2XkaW1Yz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imput_ids, attention_masks = geIdsAndMasks(df)"],"metadata":{"id":"ZL81C57LW6GP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import ElectraConfig, AutoConfig, AutoModelForSequenceClassification, BertConfig, BertModel,AutoModel"],"metadata":{"id":"ZvDmFpUUXWPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = BertConfig.from_pretrained(\"tbs17/MathBERT\", output_hidden_states=True)\n","model = BertModel.from_pretrained(\"tbs17/MathBERT\", config=config)"],"metadata":{"id":"2HM90gbkXKoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BertForSentenceClassification(BertModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        \n","        ##### START YOUR CODE HERE #####\n","        # Add a linear classifier that map BERTs [CLS] token representation to the unnormalized\n","        # output probabilities for each class (logits).\n","        # Notes: \n","        #  * See the documentation for torch.nn.Linear\n","        #  * You do not need to add a softmax, as this is included in the loss function\n","        #  * The size of BERTs token representation can be accessed at config.hidden_size\n","        #  * The number of output classes can be accessed at config.num_labels\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        ##### END YOUR CODE HERE #####\n","        self.loss = torch.nn.CrossEntropyLoss()\n","\n","    def forward(self, labels=None, **kwargs):\n","        outputs = super().forward(**kwargs)\n","        ##### START YOUR CODE HERE #####\n","        # Pass BERTs [CLS] token representation to this new classifier to produce the logits.\n","        # Notes:\n","        #  * The [CLS] token representation can be accessed at outputs.pooler_output\n","        cls_token_repr = outputs.pooler_output\n","        print(outputs.pooler_output.shape)\n","        logits = self.classifier(cls_token_repr)\n","        ##### END YOUR CODE HERE #####\n","        if labels is not None:\n","            outputs = (logits, self.loss(logits, labels))\n","        else:\n","            outputs = (logits,)\n","        return outputs"],"metadata":{"id":"Q4tscIxqdIbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class DistillBertClassifier(nn.Module):\n","    \"\"\"Bert Model for Classification Tasks.\n","    \"\"\"\n","    def __init__(self, freeze_bert=False, num_labels = 3):\n","        \"\"\"\n","        @param    bert: a BertModel object\n","        @param    classifier: a torch.nn.Module classifier\n","        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n","        \"\"\"\n","        super(DistillBertClassifier, self).__init__()\n","        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n","        D_in, H, D_out = 768, 50, num_labels\n","\n","        # Instantiate BERT model\n","        self.bert = AutoModel.from_pretrained('distilbert-base-uncased', num_labels = num_labels)\n","\n","        # Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, H),\n","            nn.ReLU(),\n","            #nn.Dropout(0.5),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","        \n","    def forward(self, input_ids, attention_mask):\n","   \n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask)\n","        \n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"],"metadata":{"id":"9dbkp89bfYrk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class ElectraModel(nn.Module):\n","    \"\"\"Bert Model for Classification Tasks.\n","    \"\"\"\n","    def __init__(self, freeze_bert=False, num_labels = 3):\n","        \"\"\"\n","        @param    bert: a BertModel object\n","        @param    classifier: a torch.nn.Module classifier\n","        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n","        \"\"\"\n","        super(ElectraModel, self).__init__()\n","        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n","        D_in, H, D_out = 768, 50, num_labels\n","\n","        # Instantiate BERT model\n","        self.bert = AutoModel.from_pretrained('google/electra-base-discriminator',num_labels=  num_labels)\n","\n","        # Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, H),\n","            nn.ReLU(),\n","            #nn.Dropout(0.5),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","        \n","    def forward(self, input_ids, attention_mask):\n","   \n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask)\n","        \n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"],"metadata":{"id":"fCTAiimXhaJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['predict_from'].isna().sum()"],"metadata":{"id":"NQ2zbK_BWEQN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def datapreprocess(df):\n","    MAX_LEN = 50\n","    bert_tokenizer = AutoTokenizer.from_pretrained('tbs17/MathBERT', do_lower_case=True)\n","    electra_tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator', do_lower_case=True)\n","    distill_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n","    inputs = df.predict_from.values\n","    labels = df.assigned_score.values\n","    indexes = df.index.values\n","    inputs = [\"[CLS] \" + text + \" [SEP]\" for text in inputs]\n","    tokenized_inputs_1 = bert_tokenizer( inputs, add_special_tokens=True, padding='max_length', max_length=MAX_LEN,\n","        return_tensors='pt', truncation=True)\n","    tokenized_inputs_2 = electra_tokenizer( inputs, add_special_tokens=True, padding='max_length', max_length=MAX_LEN,\n","        return_tensors='pt', truncation=True)\n","    tokenized_inputs_3 = distill_tokenizer( inputs, add_special_tokens=True, padding='max_length', max_length=MAX_LEN,\n","        return_tensors='pt', truncation=True)\n","\n","    input_ids_1= tokenized_inputs_1['input_ids']\n","    attention_masks_1 = tokenized_inputs_1['attention_mask']\n","\n","    input_ids_2= tokenized_inputs_2['input_ids']\n","    attention_masks_2= tokenized_inputs_2['attention_mask']\n","\n","    input_ids_3= tokenized_inputs_3['input_ids']\n","    attention_masks_3 = tokenized_inputs_3['attention_mask']\n","\n","    # Split the data into train and test sets\n","    train_inputs1, test_inputs1, train_masks1, test_masks1, train_labels, test_labels, train_indexes, test_indexes = train_test_split(\n","        input_ids_1, attention_masks_1, labels, indexes, random_state=42, test_size=0.2, stratify=labels\n","    )\n","\n","    train_inputs2, test_inputs2, train_masks2, test_masks2 = train_test_split(\n","        input_ids_2, attention_masks_2, random_state=42, test_size=0.2, stratify=labels)\n","    train_inputs3, test_inputs3, train_masks3, test_masks3,= train_test_split(\n","        input_ids_3, attention_masks_3, random_state=42, test_size=0.2, stratify=labels )\n","\n","    return train_inputs1,train_inputs2, train_inputs3,test_inputs1,test_inputs2,test_inputs3, train_masks1, train_masks2, train_masks3,test_masks1, test_masks2,test_masks3,train_labels, test_labels, test_indexes"],"metadata":{"id":"Hjg8X1LChpdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_inputs1,train_inputs2, train_inputs3,test_inputs1,test_inputs2,test_inputs3, train_masks1, train_masks2, train_masks3,test_masks1, test_masks2,test_masks3,train_labels, test_labels, test_indexes = datapreprocess(df)"],"metadata":{"id":"wnk3rv2Uj2Yl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(train_labels)\n","val_labels = torch.tensor(test_labels)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoader for our training set\n","train_data1 = TensorDataset(train_inputs1, train_masks1, train_labels)\n","train_sampler1 = RandomSampler(train_data1)\n","train_dataloader1 = DataLoader(train_data1, sampler=train_sampler1, batch_size=batch_size)\n","\n","train_data2= TensorDataset(train_inputs2, train_masks2, train_labels)\n","train_sampler2 = RandomSampler(train_data2)\n","train_dataloader2 = DataLoader(train_data2, sampler=train_sampler2, batch_size=batch_size)\n","\n","train_data3= TensorDataset(train_inputs3, train_masks3, train_labels)\n","train_sampler3= RandomSampler(train_data3)\n","train_dataloader3 = DataLoader(train_data3, sampler=train_sampler3, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data1 = TensorDataset(test_inputs1, test_masks1, val_labels)\n","val_sampler1 = SequentialSampler(val_data1)\n","val_dataloader1 = DataLoader(val_data1, sampler=val_sampler1, batch_size=batch_size)\n","\n","val_data2 = TensorDataset(test_inputs2, test_inputs2, val_labels)\n","val_sampler2 = SequentialSampler(val_data2)\n","val_dataloader2 = DataLoader(val_data2, sampler=val_sampler2, batch_size=batch_size)\n","\n","val_data3= TensorDataset(test_inputs3, test_inputs3, val_labels)\n","val_sampler3 = SequentialSampler(val_data3)\n","val_dataloader3 = DataLoader(val_data3, sampler=val_sampler3, batch_size=batch_size)"],"metadata":{"id":"wFE5tX3_j5Ef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mathbert = BertForSentenceClassification.from_pretrained(\"tbs17/MathBERT\", num_labels=df['assigned_score'].nunique()  ) \n","electrbert = ElectraModel(freeze_bert=False)\n","distill = DistillBertClassifier(freeze_bert=False)"],"metadata":{"id":"EdOvUDUeoSG2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","def initialize_model(epochs=4):\n","    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n","    \"\"\"\n","    device = 'cuda'\n","    # Instantiate Bert Classifier\n","    mathbert = BertForSentenceClassification.from_pretrained(\"tbs17/MathBERT\", num_labels=df['assigned_score'].nunique()  ) \n","    electrbert = ElectraModel(freeze_bert=False)\n","    distill = DistillBertClassifier(freeze_bert=False)\n","\n","    # Tell PyTorch to run the model on GPU\n","    mathbert.to(device)\n","    electrbert.to(device)\n","    distill.to(device)\n","\n","    # Create the optimizer\n","    optimizer1 = AdamW(mathbert.parameters(),\n","                      lr=5e-5,    # Default learning rate\n","                      eps=1e-8    # Default epsilon value\n","                      )\n","    \n","    optimizer2 = AdamW(electrbert.parameters(), lr=5e-5,  eps=1e-8   )\n","    optimizer3 = AdamW(distill.parameters(), lr=5e-5,  eps=1e-8   )\n","    \n","\n","\n","    # Total number of training steps\n","    total_steps = len(train_dataloader1) * epochs\n","\n","    # Set up the learning rate scheduler\n","    #scheduler = get_linear_schedule_with_warmup(optimizer1,\n","                                             #   num_warmup_steps=0, # Default value\n","                                              #  num_training_steps=total_steps)\n","    return mathbert, electrbert, distill,optimizer1, optimizer2, optimizer3"],"metadata":{"id":"F_ADP0womqrF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mathbert, electrbert, distill,optimizer1, optimizer2, optimizer3  = initialize_model(4)"],"metadata":{"id":"jtSYgzdNoi9O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import cohen_kappa_score"],"metadata":{"id":"fZSyxxRexeQK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5TzpoZ2bZh1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","\n","\n","# Define the loss function\n","loss_function = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Number of training epochs\n","num_epochs = 4\n","mathbert.cuda()\n","electrbert.cuda()\n","distill.cuda()\n","\n","mathbert.train()\n","electrbert.train()\n","distill.train()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    # Training phase\n","\n","    train_loss = 0\n","    train_correct = 0\n","    total_train = 0\n","    for (step1, batch1), (step2, batch2), (step3, batch3) in zip(enumerate(train_dataloader1), enumerate(train_dataloader2), enumerate(train_dataloader3)):\n","    # Your code for each step and batch goes here\n","\n","        b_input_ids1, b_attn_mask1, b_labels1 = tuple(t.to(device) for t in batch1)\n","        b_input_ids2, b_attn_mask2, b_labels2 = tuple(t.to(device) for t in batch2)\n","        b_input_ids3, b_attn_mask3, b_labels3 = tuple(t.to(device) for t in batch3)\n","        \n","        #input_ids = batch['input_ids'].to(device)\n","        #attention_mask = batch['attention_mask'].to(device)\n","        #labels = batch['labels'].to(device)\n","\n","        # Clear gradients\n","        optimizer1.zero_grad()\n","        optimizer2.zero_grad()\n","        optimizer3.zero_grad()\n","\n","        # Forward pass\n","        #logits1 = mathbert(b_input_ids1, b_attn_mask1,b_labels1)\n","        logits2 = electrbert(b_input_ids2, b_attn_mask2)\n","        #logits3 = distill(b_input_ids3, b_attn_mask3)\n","\n","        # Combine logits\n","        #concatenated_logits = torch.cat((logits2), dim=1)\n","\n","        # Calculate loss\n","        loss = loss_function(logits2, b_labels2)  # Assuming b_labels1 is used for all models\n","\n","        train_loss += loss.item()\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        #print(logits2, logits3)\n","\n","        # Calculate loss\n","        #loss2 = loss_function(logits2, b_labels2)\n","        #loss3 = loss_function(logits3, b_labels3)\n","\n","        train_loss += loss.item()\n","        \n","\n","        # Backward pass\n","        #loss2.backward()\n","        #loss3.backward()\n","\n","        # Update weights\n","        optimizer2.step()\n","        #optimizer3.step()\n","\n","        # Calculate accuracy\n","        preds = torch.argmax(logits2, dim=1).flatten()\n","        #print(preds.unique(return_counts = True))\n","        train_correct += (preds == b_labels1).sum().item()\n","        total_train += b_labels1.size(0)\n","    avg_train_loss = train_loss / len(train_dataloader1)\n","    print(avg_train_loss)\n","\n","    # Validation phase\n","    model.eval()\n","    val_accuracy = []\n","    val_loss = []\n","    total_val = 0\n","    val_predictions = []\n","    val_labels = []\n","    with torch.no_grad():\n","        all_preds = []\n","        all_labels = []\n","        for batch in val_dataloader2:\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Forward pass\n","            logits2 = electrbert(b_input_ids2, b_attn_mask2)\n","            #logits3 = distill(b_input_ids3, b_attn_mask3)\n","            concatenated_logits = torch.cat((logits2), dim=1)\n","\n","              \n","            #print(concatenated_logits.shape, b_labels.shape)\n","            # Truncate logits and labels if batch size is different\n","            #if concatenated_logits.shape[0] > b_labels.shape[0]:\n","             #   concatenated_logits = concatenated_logits[:b_labels.shape[0], :]\n","            #elif concatenated_logits.shape[0] < b_labels.shape[0]:\n","            #    b_labels = b_labels[:concatenated_logits.shape[0]]\n","            # Calculate loss\n","            loss = loss_function(concatenated_logits, b_labels)\n","            val_loss.append(loss.item())\n","\n","            # Calculate accuracy\n","            preds = torch.argmax(concatenated_logits, dim=1).flatten()\n","            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","            val_accuracy.append(accuracy)\n","            val_predictions.extend(preds.tolist())\n","            val_labels.extend(b_labels.tolist())\n","\n","    # Calculate average loss and accuracy\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","    quadratic_kappa = cohen_kappa_score(val_labels, val_predictions, weights='quadratic')\n","    # Print training progress for each epoch\n","    print(f\"Epoch {epoch+1}/{num_epochs}\")\n","    print(f\"Val Loss: {val_loss:.4f}, Kappa: {quadratic_kappa:.4f}\")\n","    print(\"--------------------\")\n"],"metadata":{"id":"NnU0J37Tqfrf"},"execution_count":null,"outputs":[]}]}