{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12657,"status":"ok","timestamp":1686081636134,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"BO1x0Sej0pvI","outputId":"116d179d-6faf-41d6-bbe0-e4c9cbc48101"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n","Requirement already satisfied: textsearch\u003e=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch\u003e=0.0.21-\u003econtractions) (0.3.2)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch\u003e=0.0.21-\u003econtractions) (2.0.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: textsearch in /usr/local/lib/python3.10/dist-packages (0.0.24)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch) (0.3.2)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch) (2.0.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["!pip install contractions\n","!pip install textsearch\n","!pip install tqdm\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686081636134,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"ghsvOKQXYT6w"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1326,"status":"ok","timestamp":1686081637457,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"gOHF695pYK9s","outputId":"e87d2db8-0d89-4381-9642-9706fdb97b62"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6601,"status":"ok","timestamp":1686081644056,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"PELwhuhPYHDe","outputId":"119e22ed-54ac-46da-ef7c-c7b62c3d47c9"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-4-c3c8f57a4438\u003e:1: DtypeWarning: Columns (10,12,15,19,20,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv('/content/drive/MyDrive/NAEP_Comp/item_feature_python_extract_Training_all.csv', encoding = 'utf8')\n"]}],"source":["df = pd.read_csv('/content/drive/MyDrive/NAEP_Comp/item_feature_python_extract_Training_all.csv', encoding = 'utf8')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8255,"status":"ok","timestamp":1686081652309,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"YzOv6Ni14lIW","outputId":"d6ede115-5fb8-4605-ff80-f89e0fc28b91"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-5-4e4dfdd30b27\u003e:17: DtypeWarning: Columns (29,30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(path + df_name + '.csv')\n"]}],"source":["# List of unique accessions\n","unique_accessions = ['VH134067', 'VH139380', 'VH266015', 'VH266510', 'VH269384',\n","                     'VH271613', 'VH302907', 'VH304954', 'VH507804', 'VH525628']\n","\n","\n","\n","# Dictionary to store the dataframes\n","dfs = {}\n","\n","# Loop through the unique accessions\n","for accession in unique_accessions:\n","    # Create the dataframe name\n","    path = '/content/drive/MyDrive/NAEP_Comp/'\n","    df_name = 'df_' + accession\n","\n","    # Read the CSV file into a dataframe\n","    df = pd.read_csv(path + df_name + '.csv')\n","\n","    # Add the dataframe to the dictionary\n","    dfs[accession] = df"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1686081652310,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"WRgG8Kyh4qZa"},"outputs":[],"source":["df = dfs['VH304954']"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686081652310,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"MewGaZzHkIuI","outputId":"b9487f5b-0ee6-49a4-8d2b-4d144bfc9b37"},"outputs":[{"data":{"text/plain":["0        Mark needs to do next is to add 5 to the answe...\n","1                             he needs to take away 43 -48\n","2         What he needs to do next is subtract 5 from 100.\n","3                          he needs to subtract 5 from 100\n","4                            he needs to solve the problem\n","                               ...                        \n","19550                                 subtract 45 from 100\n","19551    Subtract 95 from 143-43 which the answer for 1...\n","19552                     he needs to subtract 100 from 48\n","19553                                    the answer is 405\n","19554                   know he need's to subtract  143-48\n","Name: predict_from, Length: 19555, dtype: object"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df['predict_from']"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686081652310,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"T3-VVn34kz8i"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1686075635292,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"Ro4L68caSeqf"},"outputs":[],"source":["rest_texts, test_texts, rest_labels, test_labels = train_test_split(df['predict_from'], df['assigned_score'], test_size=0.1, random_state=1)\n","train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1686075635292,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"jTSgt-U-k7dJ","outputId":"b1f21cf0-bd8d-461b-89b1-de62c067dd14"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train size: 15839\n","Dev size: 1760\n","Test size: 1956\n"]}],"source":["print(\"Train size:\", len(train_texts))\n","print(\"Dev size:\", len(dev_texts))\n","print(\"Test size:\", len(test_texts))"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1686075635292,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"07cTuBbyn08j"},"outputs":[],"source":["train_labels = train_labels-1\n","dev_labels = dev_labels-1\n","test_labels = test_labels-1"]},{"cell_type":"markdown","metadata":{"id":"hjGqfbdFqyWW"},"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4170,"status":"ok","timestamp":1686081656477,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"JfcUJjkJlHND","outputId":"3ed9c50d-5463-4235-abfc-a97d5b4c04f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==4.28.0 in /usr/local/lib/python3.10/dist-packages (4.28.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.15.1)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers==4.28.0) (2023.4.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers==4.28.0) (4.5.0)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.28.0) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.28.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.28.0) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.28.0) (3.4)\n"]}],"source":["!pip install transformers==4.28.0"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686081656477,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"0CVw4re-4Ib_"},"outputs":[],"source":["MODEL = \"distilbert-base-uncased\""]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4159,"status":"ok","timestamp":1686081660634,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"cM9h4fcF4UtJ","outputId":"21432dee-d79d-46c0-e4d1-c34c958c8bde"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill\u003c0.3.7,\u003e=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec[http]\u003e=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: huggingface-hub\u003c1.0.0,\u003e=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: responses\u003c0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer\u003c4.0,\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (2.0.12)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (6.0.4)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (4.0.2)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.9.2)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.3.3)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.11.0-\u003edatasets) (3.12.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.11.0-\u003edatasets) (4.5.0)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2022.12.7)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.4)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2022.7.1)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.8.1-\u003epandas-\u003edatasets) (1.16.0)\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5723,"status":"ok","timestamp":1686081666354,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"C1iW5FB2lSVK","outputId":"a7a0ebb1-cb09-4a5e-dd87-30837b363e67"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from datasets import Dataset,load_dataset, load_from_disk, load_metric\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import TrainingArguments, Trainer, AdamW\n","\n","# Create model and tokenizer\n","# Make sure the num_labels argument matches the question (it will usually be 2, for correct/incorrect)\n","# Some questions may require more than one model (for more than one written section)\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=3)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1668,"status":"ok","timestamp":1686075651988,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"dXf-aiAP4JE3","outputId":"0f352cb4-e751-4440-edb6-2b859d6ba050"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Some questions may require more than one model (for more than one written section)\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=3)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1686081666355,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"mWdVKQNl4xqi"},"outputs":[],"source":["df['labels'] = df['score_to_predict'] - 1"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1686081666706,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"tOdT0We14spt","outputId":"8c34135b-9952-411e-f8ce-2eadb144819b"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['student_id', 'accession', 'score_to_predict', 'predict_from', 'year', 'srace10', 'dsex', 'accom2', 'iep', 'lep', 'rater_1', 'pta_rtr1', 'ptb_rtr1', 'ptc_rtr1', 'composite', 'score', 'assigned_score', 'ee_use', 'parsed_xml_v1', 'parsed_xml_v2', 'parsed_xml_v3', 'source1', 'source2', 'source3', 'source4', 'target1', 'target2', 'target3', 'target4', 'eliminations', 'selected', 'eliminated', 'selected1', 'selected2', 'selected3', 'selected4', 'eliminated1', 'eliminated2', 'eliminated3', 'eliminated4', 'selected1.1', 'selected2.1', 'eliminated1.1', 'eliminated2.1', 'partA_response_val', 'partB_response_val', 'partB_eliminations', 'predict_from_withstop', 'predict_from_onestepall', 'n_letter_all', 'n_sentence', 'n_token_verb', 'n_token_adj', 'n_token_noun', 'n_token_adv', 'n_token_content', 'n_token_content_unique', 'n_token_all', 'n_token_alpha', 'n_token_unique', 'word_diversity', 'mean_word_length', 'n_token_non_DaleChall', 'n_token_function', 'brown_popularity', 'n_token_l6', 'pr_token_l6', 'pr_token_cotent', 'equal', 'number', 'numbers', 'one', 'know', 'would', 'lines', 'input', 'older', 'get', 'output', 'phil', 'hundred', 'times', 'years', 'angles', 'add', 'must', 'sides', 'two', 'degrees', 'answer', 'zach', 'true', 'intersect', 'three', 'age', 'point', 'polygon', 'line', 'slope', 'put', 'sum', 'five', 'angle', 'years older', 'alex', 'side', 'going', 'slopes', 'dont', 'six', 'input numbers', 'must equal', 'different', 'output numbers', 'multiply', 'greater', 'intercept', 'interior', ' numb', 'numbe', 'umber', 'number.1', ' numbe', ' number', ' equa', 'equal.1', ' equal', 'inter', 'qual ', 'equal ', ' equal ', ' inte', ' inter', 'mber ', 'bers ', 'mbers', 'umber ', 'number ', ' number ', 'umbers', 'numbers.1', 'mbers ', ' numbers', 'umbers ', 'numbers ', ' line', 'angle.1', 'ould ', ' one ', ' angl', ' angle', ' woul', 'would.1', ' would', ' know', 'know ', ' phil', ' know ', ' side', 'would ', ' would ', 'input.1', ' inpu', ' time', ' input', 'ines ', ' olde', 'lines.1', 'number.2', 'triangles', 'times.1', 'degrees.1', 'numbers.2', 'three.1', 'get.1', 'add.1', 'angles.1', 'equal.2', 'subtract', 'triangle', 'input.2', 'needs', 'sum.1', 'answer.1', 'multiply.1', 'polygon.1', 'output.1', 'would.2', 'rule', 'interior.1', 'six.1', 'equals', 'one.1', 'multiplied', 'interior angles', 'mark', 'three triangles', 'must.1', 'know.1', 'sides.1', 'two.1', 'first', 'make', 'mark needs', 'put.1', 'lines.2', 'since', 'next', 'needs subtract', 'count', 'need', 'every', 'input numbers.1', 'greater.1', 'value', 'always', 'find', 'measures', 'angle.2', ' numb.1', 'umber.1', 'numbe.1', 'number.3', ' numbe.1', ' number.1', ' tria', 'trian', 'gles ', ' trian', 'ngles', 'riang', 'triang', ' triang', 'ngles ', 'angles.2', 'angles ', 'iangl', 'riangl', 'triangl', 'iangle', ' triangl', 'riangle', 'triangle.1', 'mber .1', 'umber .1', 'number .1', ' number .1', ' mult', 'multi', ' multi', 'ultip', 'multip', ' multip', 'ltipl', 'ultipl', 'multipl', ' multipl', ' equa.1', ' time.1', 'equal.3', ' equal.1', 'iangles', 'riangles', 'iangles ', 'times.2', ' times', ' degr', 'degre', 'lines.3', 'would.3', 'number.4', 'equal.4', 'slopes.1', 'subtract.1', 'intersect.1', 'add.2', 'two.2', 'numbers.3', 'slope.1', 'parallel', 'never', 'smallest', 'three.2', 'know.2', 'get.2', 'line.1', 'largest', 'needs.1', 'one.2', 'answer.2', 'lines would', 'slopes lines', 'greater.2', 'slopes equal', 'lines equal', 'two lines', 'point.1', 'biggest', 'equal would', 'never intersect', 'different.1', 'needs subtract.1', 'equal lines', 'multiply.2', 'smallest number', 'would parallel', 'count.1', 'cannot', 'years.1', 'phil.1', 'would never', 'mark.1', 'rule.1', 'value.1', 'always.1', 'least', 'still', 'first.1', ' numb.2', 'umber.2', 'numbe.2', ' numbe.2', 'number.5', ' number.2', ' line.1', ' slop', 'slope.2', ' slope', 'ines .1', 'lines.4', ' lines', 'lines ', ' lines ', 'ould .1', ' woul.1', 'would.4', ' would.1', 'would .1', ' would .1', 'inter.1', 'mber .2', 'umber .2', 'number .2', ' number .2', ' inte.1', ' subt', 'tract', 'subtr', ' subtr', 'ubtra', 'subtra', ' subtra', ' equa.2', 'btrac', 'ubtrac', 'subtrac', ' inter.1', ' subtrac', 'btract', 'ubtract', 'subtract.2', 'equal.5', ' equal.2', 'qual .1', 'equal .1', ' equal .1', 'lopes', 'slopes.2', 'text', 'text_cont', 'text_blob', 'labels'],\n","        num_rows: 15644\n","    })\n","    test: Dataset({\n","        features: ['student_id', 'accession', 'score_to_predict', 'predict_from', 'year', 'srace10', 'dsex', 'accom2', 'iep', 'lep', 'rater_1', 'pta_rtr1', 'ptb_rtr1', 'ptc_rtr1', 'composite', 'score', 'assigned_score', 'ee_use', 'parsed_xml_v1', 'parsed_xml_v2', 'parsed_xml_v3', 'source1', 'source2', 'source3', 'source4', 'target1', 'target2', 'target3', 'target4', 'eliminations', 'selected', 'eliminated', 'selected1', 'selected2', 'selected3', 'selected4', 'eliminated1', 'eliminated2', 'eliminated3', 'eliminated4', 'selected1.1', 'selected2.1', 'eliminated1.1', 'eliminated2.1', 'partA_response_val', 'partB_response_val', 'partB_eliminations', 'predict_from_withstop', 'predict_from_onestepall', 'n_letter_all', 'n_sentence', 'n_token_verb', 'n_token_adj', 'n_token_noun', 'n_token_adv', 'n_token_content', 'n_token_content_unique', 'n_token_all', 'n_token_alpha', 'n_token_unique', 'word_diversity', 'mean_word_length', 'n_token_non_DaleChall', 'n_token_function', 'brown_popularity', 'n_token_l6', 'pr_token_l6', 'pr_token_cotent', 'equal', 'number', 'numbers', 'one', 'know', 'would', 'lines', 'input', 'older', 'get', 'output', 'phil', 'hundred', 'times', 'years', 'angles', 'add', 'must', 'sides', 'two', 'degrees', 'answer', 'zach', 'true', 'intersect', 'three', 'age', 'point', 'polygon', 'line', 'slope', 'put', 'sum', 'five', 'angle', 'years older', 'alex', 'side', 'going', 'slopes', 'dont', 'six', 'input numbers', 'must equal', 'different', 'output numbers', 'multiply', 'greater', 'intercept', 'interior', ' numb', 'numbe', 'umber', 'number.1', ' numbe', ' number', ' equa', 'equal.1', ' equal', 'inter', 'qual ', 'equal ', ' equal ', ' inte', ' inter', 'mber ', 'bers ', 'mbers', 'umber ', 'number ', ' number ', 'umbers', 'numbers.1', 'mbers ', ' numbers', 'umbers ', 'numbers ', ' line', 'angle.1', 'ould ', ' one ', ' angl', ' angle', ' woul', 'would.1', ' would', ' know', 'know ', ' phil', ' know ', ' side', 'would ', ' would ', 'input.1', ' inpu', ' time', ' input', 'ines ', ' olde', 'lines.1', 'number.2', 'triangles', 'times.1', 'degrees.1', 'numbers.2', 'three.1', 'get.1', 'add.1', 'angles.1', 'equal.2', 'subtract', 'triangle', 'input.2', 'needs', 'sum.1', 'answer.1', 'multiply.1', 'polygon.1', 'output.1', 'would.2', 'rule', 'interior.1', 'six.1', 'equals', 'one.1', 'multiplied', 'interior angles', 'mark', 'three triangles', 'must.1', 'know.1', 'sides.1', 'two.1', 'first', 'make', 'mark needs', 'put.1', 'lines.2', 'since', 'next', 'needs subtract', 'count', 'need', 'every', 'input numbers.1', 'greater.1', 'value', 'always', 'find', 'measures', 'angle.2', ' numb.1', 'umber.1', 'numbe.1', 'number.3', ' numbe.1', ' number.1', ' tria', 'trian', 'gles ', ' trian', 'ngles', 'riang', 'triang', ' triang', 'ngles ', 'angles.2', 'angles ', 'iangl', 'riangl', 'triangl', 'iangle', ' triangl', 'riangle', 'triangle.1', 'mber .1', 'umber .1', 'number .1', ' number .1', ' mult', 'multi', ' multi', 'ultip', 'multip', ' multip', 'ltipl', 'ultipl', 'multipl', ' multipl', ' equa.1', ' time.1', 'equal.3', ' equal.1', 'iangles', 'riangles', 'iangles ', 'times.2', ' times', ' degr', 'degre', 'lines.3', 'would.3', 'number.4', 'equal.4', 'slopes.1', 'subtract.1', 'intersect.1', 'add.2', 'two.2', 'numbers.3', 'slope.1', 'parallel', 'never', 'smallest', 'three.2', 'know.2', 'get.2', 'line.1', 'largest', 'needs.1', 'one.2', 'answer.2', 'lines would', 'slopes lines', 'greater.2', 'slopes equal', 'lines equal', 'two lines', 'point.1', 'biggest', 'equal would', 'never intersect', 'different.1', 'needs subtract.1', 'equal lines', 'multiply.2', 'smallest number', 'would parallel', 'count.1', 'cannot', 'years.1', 'phil.1', 'would never', 'mark.1', 'rule.1', 'value.1', 'always.1', 'least', 'still', 'first.1', ' numb.2', 'umber.2', 'numbe.2', ' numbe.2', 'number.5', ' number.2', ' line.1', ' slop', 'slope.2', ' slope', 'ines .1', 'lines.4', ' lines', 'lines ', ' lines ', 'ould .1', ' woul.1', 'would.4', ' would.1', 'would .1', ' would .1', 'inter.1', 'mber .2', 'umber .2', 'number .2', ' number .2', ' inte.1', ' subt', 'tract', 'subtr', ' subtr', 'ubtra', 'subtra', ' subtra', ' equa.2', 'btrac', 'ubtrac', 'subtrac', ' inter.1', ' subtrac', 'btract', 'ubtract', 'subtract.2', 'equal.5', ' equal.2', 'qual .1', 'equal .1', ' equal .1', 'lopes', 'slopes.2', 'text', 'text_cont', 'text_blob', 'labels'],\n","        num_rows: 3911\n","    })\n","})"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Convert to dataset format\n","dataset = Dataset.from_pandas(df, preserve_index=False)\n","dataset = dataset.train_test_split(test_size=0.2, seed=11)\n","dataset"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":58634,"status":"ok","timestamp":1686081725333,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"JHrViSy-4oVq","outputId":"9d8d8b23-efca-419f-a59b-281e13c793ff"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb62e26a49f6482d9048823d8a96e176","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/15644 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17caae589aac4bb395928a53e4dfeb64","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3911 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Tokenize the data\n","model.resize_token_embeddings(len(tokenizer))\n","\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"parsed_xml_v1\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686081754434,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"42Pf5JMQPtMb"},"outputs":[],"source":["import sklearn\n","import numpy as np"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1686081758454,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"FfdcrlbXPi2r"},"outputs":[],"source":["# We'll use these weights later on to make up for the slightly imbalanced dataset\n","classes = np.unique(df[\"labels\"])\n","class_weights = sklearn.utils.class_weight.compute_class_weight(\n","    \"balanced\", classes=classes, y=df[\"labels\"]\n",")\n","\n","class_weights = {clazz : weight for clazz, weight in zip(classes, class_weights)}"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1686081759754,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"YvkXkFEMPvZR","outputId":"ecf0d032-4910-48d2-e746-0cf423dec531"},"outputs":[{"data":{"text/plain":["{0: 0.9187221047686164, 1: 0.7314108318372232, 2: 1.8371852686959789}"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["class_weights"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4258,"status":"ok","timestamp":1686081769019,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"N_dlenrI5MFJ","outputId":"5747f85d-4fbe-4880-d0fe-9a846ca227eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.19.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch\u003e=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.6.0-\u003eaccelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.6.0-\u003eaccelerate) (16.0.5)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.6.0-\u003eaccelerate) (2.1.2)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.6.0-\u003eaccelerate) (1.3.0)\n"]}],"source":["pip install --upgrade accelerate"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"executionInfo":{"elapsed":2362802,"status":"ok","timestamp":1686078081622,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"CB_gWEfY5JJp","outputId":"b29fc516-310a-419b-e75f-fe2361bb1b05"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='1467' max='1467' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [1467/1467 39:18, Epoch 3/3]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eEpoch\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0.644500\u003c/td\u003e\n","      \u003ctd\u003e0.573351\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0.561400\u003c/td\u003e\n","      \u003ctd\u003e0.561390\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e0.509700\u003c/td\u003e\n","      \u003ctd\u003e0.576746\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1467, training_loss=0.5718707999324993, metrics={'train_runtime': 2360.9083, 'train_samples_per_second': 19.879, 'train_steps_per_second': 0.621, 'total_flos': 6217070824378368.0, 'train_loss': 0.5718707999324993, 'epoch': 3.0})"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# AdamW Training\n","training_args = TrainingArguments(output_dir=\"test_trainer\",\n","                                  logging_strategy=\"epoch\",\n","                                  evaluation_strategy=\"epoch\",\n","                                  per_device_train_batch_size=32,\n","                                  per_device_eval_batch_size=32,\n","                                  num_train_epochs=3,\n","                                  save_total_limit = 2,\n","                                  save_strategy = 'no',\n","                                  load_best_model_at_end=False\n","                                  )\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    compute_metrics=None,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":71480,"status":"ok","timestamp":1686078164615,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"en2V9ZQ-FWbT","outputId":"f58cf87e-fe32-4031-b041-8a89d43c61c3"},"outputs":[{"data":{"text/html":[],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["# Get predictions for parts graded via model\n","pred, actual, _ = trainer.predict(tokenized_datasets['test'])"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686078247303,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"i5ye7vQEF0tf","outputId":"62290991-db8e-47aa-fbe6-1bc46f2a992e"},"outputs":[{"data":{"text/plain":["array([0, 0, 1, ..., 1, 0, 1])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["actual"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":251,"status":"ok","timestamp":1686078296632,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"pq24ApXHFh78"},"outputs":[],"source":["# Convert probabilities to class labels\n","pred_labels = np.argmax(pred, axis=1)\n","\n","# Calculate the quadratic weighted Cohen's kappa score\n","kappa = cohen_kappa_score(actual, pred_labels, weights='quadratic')"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":361,"status":"ok","timestamp":1686078307449,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"oclZtTmWGS31","outputId":"ef8ccaf9-0220-43f6-f8cc-020384e2e083"},"outputs":[{"data":{"text/plain":["0.6964706160407158"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["kappa"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1686081778395,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"lm5aMwgBsNhp"},"outputs":[],"source":["from transformers import BertModel\n","import torch.nn as nn\n","\n","class BertForSentenceClassification(BertModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        \n","        ##### START YOUR CODE HERE #####\n","        # Add a linear classifier that map BERTs [CLS] token representation to the unnormalized\n","        # output probabilities for each class (logits).\n","        # Notes: \n","        #  * See the documentation for torch.nn.Linear\n","        #  * You do not need to add a softmax, as this is included in the loss function\n","        #  * The size of BERTs token representation can be accessed at config.hidden_size\n","        #  * The number of output classes can be accessed at config.num_labels\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        ##### END YOUR CODE HERE #####\n","        self.loss = torch.nn.CrossEntropyLoss()\n","\n","    def forward(self, labels=None, **kwargs):\n","        outputs = super().forward(**kwargs)\n","        ##### START YOUR CODE HERE #####\n","        # Pass BERTs [CLS] token representation to this new classifier to produce the logits.\n","        # Notes:\n","        #  * The [CLS] token representation can be accessed at outputs.pooler_output\n","        cls_token_repr = outputs.pooler_output\n","        print(outputs.pooler_output.shape)\n","        logits = self.classifier(cls_token_repr)\n","        ##### END YOUR CODE HERE #####\n","        if labels is not None:\n","            outputs = (logits, self.loss(logits, labels))\n","        else:\n","            outputs = (logits,)\n","        return outputs"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686081780872,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"nIvm0ezezQTc"},"outputs":[],"source":["from torch.optim import AdamW\n","import time\n","import datetime\n","from transformers import get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import cohen_kappa_score\n","\n","\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","def get_optimizer_and_scheduler(model, total_steps, lr=2e-5, weight_decay=0.01):\n","    # Apply weight decay to all parameters beside the biases or LayerNorm weights\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {\n","            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            'weight_decay': weight_decay},\n","        {\n","            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            'weight_decay': 0.0\n","        }\n","    ]\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        # Warmup learning rate for first 10% of training steps\n","        num_warmup_steps=int(0.10 * total_steps), \n","        num_training_steps=total_steps,\n","    )\n","    return optimizer, scheduler\n","\n","def train_model(model, epochs, train_dataloader, validation_dataloader,test_indexs):\n","    # Use GPU, if available\n","    device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    kappa_score_int = 0\n","\n","    # Setup optimizer and LR scheduler \n","    total_steps = len(train_dataloader) * epochs\n","    optimizer, scheduler = get_optimizer_and_scheduler(\n","        model, total_steps, lr=5e-5, weight_decay=0.01\n","    )\n","\n","    loss_values = []\n","    eval_accs = []\n","        # Tracking variables\n","    \n","\n","    for epoch in range(0, epochs):\n","        t0 = time.time()\n","\n","        total_loss = 0\n","        model.train()\n","        predictions, true_labels = [], []\n","\n","        with tqdm(train_dataloader, unit=\"batch\") as train_pbar:\n","            for batch in train_pbar:\n","                train_pbar.set_description(f\"Training (epoch {epoch + 1})\")\n","                b_input_ids = batch[0].to(device)\n","                b_input_mask = batch[1].to(device)\n","                b_labels = batch[2].to(device)\n","\n","                model.zero_grad()        \n","\n","                # Perform a forward pass (evaluate the model on this training batch).\n","                # This will return the loss because we have provided the `labels`.\n","                outputs = model(\n","                    input_ids=b_input_ids, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels\n","                )\n","                \n","                # The call to `model` always returns a tuple, so we need to pull the \n","                # loss value out of the tuple.\n","                _, loss = outputs\n","\n","                # Accumulate the training loss over all of the batches so that we can\n","                # calculate the average loss at the end. `loss` is a Tensor containing a\n","                # single value; the `.item()` function just returns the Python value \n","                # from the tensor.\n","                total_loss += loss.item()\n","\n","                # Perform a backward pass to calculate the gradients.\n","                loss.backward()\n","\n","                # Clip the norm of the gradients to 1.0.\n","                # This is to help prevent the \"exploding gradients\" problem.\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","                # Update parameters and take a step using the computed gradient.\n","                # The optimizer dictates the \"update rule\"--how the parameters are\n","                # modified based on their gradients, the learning rate, etc.\n","                optimizer.step()\n","\n","                # Update the learning rate.\n","                scheduler.step()\n","\n","        # Calculate the average loss over the training data.\n","        avg_train_loss = total_loss / len(train_dataloader)            \n","        \n","        # Store the loss value for plotting the learning curve.\n","        loss_values.append(avg_train_loss)\n","\n","        print(\"  * Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  * Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","            \n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","        model.eval()\n","\n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_steps, nb_eval_examples = 0, 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","            \n","            with torch.no_grad():        \n","                # Forward pass, calculate logit predictions.\n","                # This will return the logits rather than the loss because we have\n","                # not provided labels.\n","                # token_type_ids is the same as the \"segment ids\", which \n","                # differentiates sentence 1 and 2 in 2-sentence tasks.\n","                outputs = model(\n","                    input_ids=b_input_ids, \n","                    attention_mask=b_input_mask\n","                )\n","            \n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            logits = outputs[0]\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","            # Calculate the accuracy for this batch of test sentences.\n","            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","            # Accumulate the total accuracy.\n","            eval_accuracy += tmp_eval_accuracy\n","            # Track the number of batches\n","            nb_eval_steps += 1\n","            predictions.append(logits)\n","            true_labels.append(label_ids)\n","       \n","        predictions = np.concatenate(predictions)\n","        true_labels = np.concatenate(true_labels)\n","\n","        predicted_labels = np.argmax(predictions, axis=1)\n","        kappa_score = cohen_kappa_score(true_labels, predicted_labels, labels=None, weights= 'quadratic', sample_weight=None)\n","        if kappa_score \u003e kappa_score_int :\n","          kappa_score_int = kappa_score\n","          results_df = pd.DataFrame(index=test_indexs)\n","          results_df['indexes'] = test_indexs\n","          results_df['TrueValue'] = true_labels\n","          results_df['PredictedValue'] = predicted_labels\n","\n","        \n","\n","        avg_eval_acc = eval_accuracy/nb_eval_steps\n","        print(\"  * Accuracy: {0:.2f}\".format(avg_eval_acc))\n","        print(\"  * Validation took: {:}\".format(format_time(time.time() - t0)))\n","        print(\"Kappa Score = \" + str(kappa_score))\n","        eval_accs.append(avg_eval_acc)\n","    print(\"Training complete!\")\n","    return loss_values, eval_accs, results_df, kappa_score_int"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1686081836739,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"bPepuHRLEYST"},"outputs":[],"source":["def datapreprocess(df):\n","    MAX_LEN = 180\n","    bert_tokenizer = AutoTokenizer.from_pretrained('tbs17/MathBERT', do_lower_case=True)\n","    inputs = df.predict_from_withstop.values\n","    labels = df.assigned_score.values\n","    indexes = df.index.values\n","    inputs = [\"[CLS] \" + text + \" [SEP]\" for text in inputs]\n","    tokenized_inputs = bert_tokenizer(\n","        inputs,\n","        add_special_tokens=True,\n","        padding='max_length',\n","        max_length=MAX_LEN,\n","        return_tensors='pt',\n","        truncation=True\n","    )\n","\n","    input_ids = tokenized_inputs['input_ids']\n","    attention_masks = tokenized_inputs['attention_mask']\n","\n","    # Split the data into train and test sets\n","    train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels, train_indexes, test_indexes = train_test_split(\n","        input_ids, attention_masks, labels, indexes, random_state=42, test_size=0.2, stratify=labels\n","    )\n","\n","    return train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels, test_indexes\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":263,"status":"ok","timestamp":1686081839203,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"Nczw5rdMrN7o"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","def train_valid_split(train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels, test_indexes, batch_size=32 ):\n","    # Use 80% for training and 20% for validation.\n","\n","    train_labels = torch.tensor(train_labels)\n","    validation_labels = torch.tensor(test_labels)\n","\n","    # Create the DataLoader for our training set.\n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","\n","    # Create the DataLoader for our validation set.\n","    validation_data = TensorDataset(test_inputs, test_masks, validation_labels)\n","    validation_dataloader = DataLoader(validation_data, shuffle=False, batch_size=batch_size)\n","\n","    return train_dataloader, validation_dataloader"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1686081880635,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"he3OyB0kcfQC"},"outputs":[],"source":["from transformers import BertModel\n","import torch.nn as nn\n","\n","class BertCNNLSTMClassifier(BertModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        \n","        self.lstm_hidden_size = 256\n","        self.lstm_num_layers = 4\n","        self.cnn_num_filters = 256\n","        self.cnn_kernel_size = 3\n","        \n","        # Add a linear classifier that maps the concatenated representation to the unnormalized\n","        # output probabilities for each class (logits).\n","        self.classifier = nn.Linear(config.hidden_size + self.lstm_hidden_size * 2 + self.cnn_num_filters, config.num_labels)\n","        \n","        # Bidirectional LSTM layer\n","        self.lstm = nn.LSTM(config.hidden_size, self.lstm_hidden_size, num_layers=self.lstm_num_layers, batch_first=True, bidirectional=True)\n","        \n","        # CNN layer\n","        self.cnn = nn.Conv1d(\n","            in_channels=self.lstm_hidden_size * 2,\n","            out_channels=self.cnn_num_filters,\n","            kernel_size=self.cnn_kernel_size,\n","            padding=(self.cnn_kernel_size - 1) // 2\n","        )\n","        \n","        # Activation function\n","        self.activation = nn.ReLU()\n","        \n","        # Loss function\n","        self.loss = torch.nn.CrossEntropyLoss()\n","\n","    def forward(self, labels=None, **kwargs):\n","        outputs = super().forward(**kwargs)\n","        \n","        # Pass BERT's [CLS] token representation to the LSTM layer\n","        cls_token_repr = outputs.pooler_output\n","        lstm_output, _ = self.lstm(cls_token_repr.unsqueeze(1))\n","        \n","        # Reshape the LSTM output for CNN\n","        lstm_output = lstm_output.permute(0, 2, 1)  # Reshape for CNN\n","        \n","        # Pass the LSTM output through the CNN layer\n","        cnn_output = self.cnn(lstm_output)\n","        cnn_output = cnn_output.squeeze(2)  # Remove the last dimension\n","        \n","        # Reshape the LSTM output for concatenation\n","        lstm_output = lstm_output.squeeze(2)\n","        \n","        # Concatenate the [CLS] token representation, LSTM output, and CNN output\n","        concatenated_repr = torch.cat((cls_token_repr, lstm_output, cnn_output), dim=1)\n","        \n","        # Pass the concatenated representation to the linear classifier\n","        logits = self.classifier(concatenated_repr)\n","        \n","        if labels is not None:\n","            outputs = (logits, self.loss(logits, labels))\n","        else:\n","            outputs = (logits,)\n","        \n","        return outputs\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2029,"status":"ok","timestamp":1686081885088,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"wiIBd3H9cd-Q","outputId":"8ec393a0-d061-446c-ea2e-43e31df3e459"},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertCNNLSTMClassifier: ['distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias']\n","- This IS expected if you are initializing BertCNNLSTMClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertCNNLSTMClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertCNNLSTMClassifier were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['lstm.weight_hh_l1', 'encoder.layer.7.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'lstm.weight_ih_l3', 'encoder.layer.7.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'lstm.bias_ih_l0', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'cnn.weight', 'lstm.weight_hh_l0_reverse', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'pooler.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'lstm.weight_hh_l2', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'lstm.bias_hh_l1', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'lstm.bias_hh_l2', 'encoder.layer.10.attention.self.value.bias', 'lstm.bias_hh_l0_reverse', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.9.attention.self.query.weight', 'lstm.bias_hh_l1_reverse', 'encoder.layer.1.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'lstm.bias_ih_l1_reverse', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'lstm.bias_ih_l1', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'lstm.bias_ih_l2_reverse', 'lstm.weight_hh_l2_reverse', 'classifier.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'lstm.bias_ih_l0_reverse', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'lstm.weight_hh_l3', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'lstm.weight_ih_l1_reverse', 'lstm.weight_hh_l3_reverse', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'lstm.bias_hh_l3', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'lstm.bias_ih_l2', 'lstm.bias_ih_l3_reverse', 'encoder.layer.2.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l1_reverse', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'lstm.weight_ih_l3_reverse', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'lstm.weight_ih_l1', 'cnn.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'lstm.weight_ih_l0_reverse', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'lstm.bias_hh_l0', 'encoder.layer.11.attention.output.dense.bias', 'lstm.bias_hh_l3_reverse', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'lstm.bias_ih_l3', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'classifier.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'lstm.weight_ih_l2_reverse', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'lstm.weight_ih_l2', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'lstm.weight_hh_l0', 'lstm.bias_hh_l2_reverse', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = BertCNNLSTMClassifier.from_pretrained(\n","    \"distilbert-base-uncased\",  # the name of the pretrained model\n","    num_labels=df['labels'].nunique(),  ) "]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":7821,"status":"ok","timestamp":1686081892908,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"n8Wjx6d-QPbh"},"outputs":[],"source":["train_inputs = tokenized_datasets['train']['input_ids']\n","train_masks = tokenized_datasets['train']['attention_mask'] \n","train_labels = tokenized_datasets['train']['labels']\n","test_inputs = tokenized_datasets['test']['input_ids']\n","test_masks = tokenized_datasets['test']['attention_mask'] \n","test_labels = tokenized_datasets['test']['labels']\n","test_indexes = tokenized_datasets['test']['student_id']"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":1684,"status":"ok","timestamp":1686081895299,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"NC_xYd0URnpl"},"outputs":[],"source":["# Convert data to tensors\n","train_inputs = torch.tensor(train_inputs)\n","train_masks = torch.tensor(train_masks)\n","train_labels = torch.tensor(train_labels)\n","test_inputs = torch.tensor(test_inputs)\n","test_masks = torch.tensor(test_masks)\n","test_labels = torch.tensor(test_labels)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1686081895300,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"Sfla7SSPQlay","outputId":"e1bfded7-86e0-4284-93ed-3a2c7fa1db48"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-25-94c433efbdff\u003e:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_labels = torch.tensor(train_labels)\n","\u003cipython-input-25-94c433efbdff\u003e:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  validation_labels = torch.tensor(test_labels)\n"]}],"source":["train_dataloader,validation_dataloader= train_valid_split(train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels, test_indexes )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0Q_LoYF_SHrk"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training (epoch 1):   0%|          | 1/489 [01:45\u003c14:16:00, 105.25s/batch]\n"]},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e\u003cspan style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"\u003eTraceback \u003c/span\u003e\u003cspan style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\"\u003e(most recent call last)\u003c/span\u003e\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003e\u0026lt;cell line: 1\u0026gt;\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e1\u003c/span\u003e                                                                              \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003etrain_model\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e92\u003c/span\u003e                                                                                \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003e/usr/local/lib/python3.10/dist-packages/torch/\u003c/span\u003e\u003cspan style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"\u003e_tensor.py\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e487\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003ebackward\u003c/span\u003e                         \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 484             \u003c/span\u003ecreate_graph=create_graph,                                                \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 485             \u003c/span\u003einputs=inputs,                                                            \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 486          \u003c/span\u003e)                                                                             \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e 487 \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e      \u003c/span\u003etorch.autograd.backward(                                                          \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 488          \u003c/span\u003e\u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e, gradient, retain_graph, create_graph, inputs=inputs                     \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 489       \u003c/span\u003e)                                                                                 \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 490 \u003c/span\u003e                                                                                          \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003e/usr/local/lib/python3.10/dist-packages/torch/autograd/\u003c/span\u003e\u003cspan style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"\u003e__init__.py\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e200\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003ebackward\u003c/span\u003e               \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e197    # The reason we repeat same the comment below is that\u003c/span\u003e                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e198    # some Python versions print out the first line of a multi-line function\u003c/span\u003e               \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e199    # calls in the traceback and some print out the last line\u003c/span\u003e                              \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e200 \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e   \u003c/span\u003eVariable._execution_engine.run_backward(  \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e# Calls into the C++ engine to run the bac\u003c/span\u003e   \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e201       \u003c/span\u003etensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e202       \u003c/span\u003eallow_unreachable=\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eTrue\u003c/span\u003e, accumulate_grad=\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eTrue\u003c/span\u003e)  \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e# Calls into the C++ engine to ru\u003c/span\u003e   \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e203 \u003c/span\u003e                                                                                           \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\"\u003eKeyboardInterrupt\u003c/span\u003e\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m in \u001b[92m\u003ccell line: 1\u003e\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m in \u001b[92mtrain_model\u001b[0m:\u001b[94m92\u001b[0m                                                                                \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m            \u001b[0mcreate_graph=create_graph,                                                \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m            \u001b[0minputs=inputs,                                                            \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m         \u001b[0m)                                                                             \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[31m \u001b[0m 487 \u001b[2m      \u001b[0mtorch.autograd.backward(                                                          \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[31m \u001b[0m200 \u001b[2m   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m      \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m      \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m\n","\u001b[1;91mKeyboardInterrupt\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["mathbert_loss_vals, mathbert_eval_accs, results_df, kappa_score = train_model(model=model,\n","    epochs=2,\n","    train_dataloader=train_dataloader,\n","    validation_dataloader=validation_dataloader, test_indexs=test_indexes)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1655,"status":"error","timestamp":1686078722195,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"g1zWkWcTHpf1","outputId":"6c560b72-fbd9-4d59-c60d-82a39dc7a86e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e\u003cspan style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"\u003eTraceback \u003c/span\u003e\u003cspan style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\"\u003e(most recent call last)\u003c/span\u003e\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003e\u0026lt;cell line: 19\u0026gt;\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e19\u003c/span\u003e                                                                            \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003e/usr/local/lib/python3.10/dist-packages/transformers/\u003c/span\u003e\u003cspan style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"\u003etrainer.py\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e1662\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003etrain\u003c/span\u003e                    \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1659       \u003c/span\u003einner_training_loop = find_executable_batch_size(                                 \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1660          \u003c/span\u003e\u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e._inner_training_loop, \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e._train_batch_size, args.auto_find_batch_size  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1661       \u003c/span\u003e)                                                                                 \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e1662 \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e      \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003ereturn\u003c/span\u003e inner_training_loop(                                                       \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1663          \u003c/span\u003eargs=args,                                                                    \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1664          \u003c/span\u003eresume_from_checkpoint=resume_from_checkpoint,                                \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1665          \u003c/span\u003etrial=trial,                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003e/usr/local/lib/python3.10/dist-packages/transformers/\u003c/span\u003e\u003cspan style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"\u003etrainer.py\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e1929\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003e_inner_training_loop\u003c/span\u003e     \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1926                \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003ewith\u003c/span\u003e model.no_sync():                                                 \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1927                   \u003c/span\u003etr_loss_step = \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e.training_step(model, inputs)                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1928             \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eelse\u003c/span\u003e:                                                                     \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e1929 \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e               \u003c/span\u003etr_loss_step = \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e.training_step(model, inputs)                      \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1930             \u003c/span\u003e                                                                          \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1931             \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eif\u003c/span\u003e (                                                                      \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1932                \u003c/span\u003eargs.logging_nan_inf_filter                                           \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003e/usr/local/lib/python3.10/dist-packages/transformers/\u003c/span\u003e\u003cspan style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"\u003etrainer.py\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e2699\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003etraining_step\u003c/span\u003e            \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2696          \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003ereturn\u003c/span\u003e loss_mb.reduce_mean().detach().to(\u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e.args.device)                    \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2697       \u003c/span\u003e                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2698       \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003ewith\u003c/span\u003e \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e.compute_loss_context_manager():                                         \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e2699 \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e         \u003c/span\u003eloss = \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e.compute_loss(model, inputs)                                       \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2700       \u003c/span\u003e                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2701       \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eif\u003c/span\u003e \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e.args.n_gpu \u0026gt; \u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e1\u003c/span\u003e:                                                           \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2702          \u003c/span\u003eloss = loss.mean()  \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e# mean() to average on multi-gpu parallel training\u003c/span\u003e        \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003e/usr/local/lib/python3.10/dist-packages/transformers/\u003c/span\u003e\u003cspan style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"\u003etrainer.py\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e2731\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003ecompute_loss\u003c/span\u003e             \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2728          \u003c/span\u003elabels = inputs.pop(\u003cspan style=\"color: #808000; text-decoration-color: #808000\"\u003e\"labels\"\u003c/span\u003e)                                                 \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2729       \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eelse\u003c/span\u003e:                                                                             \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2730          \u003c/span\u003elabels = \u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eNone\u003c/span\u003e                                                                 \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e2731 \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e      \u003c/span\u003eoutputs = model(**inputs)                                                         \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2732       # Save past state if it exists\u003c/span\u003e                                                    \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2733       # TODO: this needs to be fixed and made cleaner later.\u003c/span\u003e                            \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e2734       \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eif\u003c/span\u003e \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e.args.past_index \u0026gt;= \u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e0\u003c/span\u003e:                                                     \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003e/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u003c/span\u003e\u003cspan style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"\u003emodule.py\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e1501\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003e_call_impl\u003c/span\u003e            \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1498       \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eif\u003c/span\u003e \u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003enot\u003c/span\u003e (\u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e._backward_hooks \u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003eor\u003c/span\u003e \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e._backward_pre_hooks \u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003eor\u003c/span\u003e \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eself\u003c/span\u003e._forward_hooks   \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1499             \u003c/span\u003e\u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003eor\u003c/span\u003e _global_backward_pre_hooks \u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003eor\u003c/span\u003e _global_backward_hooks                   \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1500             \u003c/span\u003e\u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003eor\u003c/span\u003e _global_forward_hooks \u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003eor\u003c/span\u003e _global_forward_pre_hooks):                   \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e1501 \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e         \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003ereturn\u003c/span\u003e forward_call(*args, **kwargs)                                          \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1502       # Do not call functions when jit is used\u003c/span\u003e                                          \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1503       \u003c/span\u003efull_backward_hooks, non_full_backward_hooks = [], []                             \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e1504       \u003c/span\u003ebackward_pre_hooks = []                                                           \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003eforward\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e35\u003c/span\u003e                                                                                    \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003e/usr/local/lib/python3.10/dist-packages/transformers/models/bert/\u003c/span\u003e\u003cspan style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"\u003emodeling_bert.py\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e972\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003eforward\u003c/span\u003e \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e                                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 969       \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eelif\u003c/span\u003e inputs_embeds \u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003eis\u003c/span\u003e \u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003enot\u003c/span\u003e \u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eNone\u003c/span\u003e:                                                   \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 970          \u003c/span\u003einput_shape = inputs_embeds.size()[:-\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e1\u003c/span\u003e]                                       \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 971       \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eelse\u003c/span\u003e:                                                                             \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e 972 \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e         \u003c/span\u003e\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eraise\u003c/span\u003e \u003cspan style=\"color: #00ffff; text-decoration-color: #00ffff\"\u003eValueError\u003c/span\u003e(\u003cspan style=\"color: #808000; text-decoration-color: #808000\"\u003e\"You have to specify either input_ids or inputs_embeds\"\u003c/span\u003e)     \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 973       \u003c/span\u003e                                                                                  \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 974       \u003c/span\u003ebatch_size, seq_length = input_shape                                              \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e   \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e 975       \u003c/span\u003edevice = input_ids.device \u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eif\u003c/span\u003e input_ids \u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003eis\u003c/span\u003e \u003cspan style=\"color: #ff00ff; text-decoration-color: #ff00ff\"\u003enot\u003c/span\u003e \u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eNone\u003c/span\u003e \u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003eelse\u003c/span\u003e inputs_embeds.device      \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\"\u003eValueError: \u003c/span\u003eYou have to specify either input_ids or inputs_embeds\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m in \u001b[92m\u003ccell line: 19\u003e\u001b[0m:\u001b[94m19\u001b[0m                                                                            \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2m      \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[31m \u001b[0m1662 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m         \u001b[0margs=args,                                                                    \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2m         \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m         \u001b[0mtrial=trial,                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1929\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1926 \u001b[0m\u001b[2m               \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1927 \u001b[0m\u001b[2m                  \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1928 \u001b[0m\u001b[2m            \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[31m \u001b[0m1929 \u001b[2m               \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1930 \u001b[0m\u001b[2m            \u001b[0m                                                                          \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1931 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1932 \u001b[0m\u001b[2m               \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2699\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2696 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m loss_mb.reduce_mean().detach().to(\u001b[96mself\u001b[0m.args.device)                    \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2697 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2698 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.compute_loss_context_manager():                                         \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[31m \u001b[0m2699 \u001b[2m         \u001b[0mloss = \u001b[96mself\u001b[0m.compute_loss(model, inputs)                                       \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2700 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2701 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.n_gpu \u003e \u001b[94m1\u001b[0m:                                                           \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2702 \u001b[0m\u001b[2m         \u001b[0mloss = loss.mean()  \u001b[2m# mean() to average on multi-gpu parallel training\u001b[0m        \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2731\u001b[0m in \u001b[92mcompute_loss\u001b[0m             \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2728 \u001b[0m\u001b[2m         \u001b[0mlabels = inputs.pop(\u001b[33m\"\u001b[0m\u001b[33mlabels\u001b[0m\u001b[33m\"\u001b[0m)                                                 \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2729 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2730 \u001b[0m\u001b[2m         \u001b[0mlabels = \u001b[94mNone\u001b[0m                                                                 \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[31m \u001b[0m2731 \u001b[2m      \u001b[0moutputs = model(**inputs)                                                         \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2732 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Save past state if it exists\u001b[0m                                                    \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2733 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# TODO: this needs to be fixed and made cleaner later.\u001b[0m                            \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m2734 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.past_index \u003e= \u001b[94m0\u001b[0m:                                                     \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[31m \u001b[0m1501 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m      \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m      \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m35\u001b[0m                                                                                    \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/\u001b[0m\u001b[1;33mmodeling_bert.py\u001b[0m:\u001b[94m972\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 969 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melif\u001b[0m inputs_embeds \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                   \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 970 \u001b[0m\u001b[2m         \u001b[0minput_shape = inputs_embeds.size()[:-\u001b[94m1\u001b[0m]                                       \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 971 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m \u001b[31m \u001b[0m 972 \u001b[2m         \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mYou have to specify either input_ids or inputs_embeds\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 973 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 974 \u001b[0m\u001b[2m      \u001b[0mbatch_size, seq_length = input_shape                                              \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m   \u001b[2m 975 \u001b[0m\u001b[2m      \u001b[0mdevice = input_ids.device \u001b[94mif\u001b[0m input_ids \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m inputs_embeds.device      \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m\n","\u001b[1;91mValueError: \u001b[0mYou have to specify either input_ids or inputs_embeds\n"]},"metadata":{},"output_type":"display_data"}],"source":["# AdamW Training\n","training_args = TrainingArguments(output_dir=\"test_trainer\",\n","                                  logging_strategy=\"epoch\",\n","                                  evaluation_strategy=\"epoch\",\n","                                  per_device_train_batch_size=32,\n","                                  per_device_eval_batch_size=32,\n","                                  num_train_epochs=3,\n","                                  save_total_limit = 2,\n","                                  save_strategy = 'no',\n","                                  load_best_model_at_end=False\n","                                  )\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    compute_metrics=None,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1686078081624,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"Aa7KbrIQE_fc"},"outputs":[],"source":["def train(df, name) :\n","  train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels, test_indexes = datapreprocess(df)\n","  train_dataloader,validation_dataloader= train_valid_split(train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels, test_indexes )\n","  mathbert = BertCNNLSTMClassifier.from_pretrained(\n","    \"tbs17/MathBERT\",  # the name of the pretrained model\n","    num_labels=df['assigned_score'].nunique()+1,  ) \n","  mathbert_loss_vals, mathbert_eval_accs, results_df, kappa_score = train_model(model=mathbert,\n","    epochs=6,\n","    train_dataloader=train_dataloader,\n","    validation_dataloader=validation_dataloader, test_indexs=test_indexes)\n","  PATH1 = \"/content/drive/MyDrive/NAEP_Comp/Bert+CNN/\" + name  +\".pt\"\n","  #torch.save(mathbert, PATH1)\n","  results_df.to_csv('/content/drive/MyDrive/NAEP_Comp/MathBert+CNN/' + str(name) + '.csv', index=False)\n","  return kappa_score\n","   # the number of classes in our downstream task \n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":485,"status":"error","timestamp":1686078082102,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"xBYBCSt3GKIH","outputId":"160d06dd-008d-4453-d00a-650dc6045997"},"outputs":[{"name":"stdout","output_type":"stream","text":["VH304954\n"]},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e\u003cspan style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"\u003eTraceback \u003c/span\u003e\u003cspan style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\"\u003e(most recent call last)\u003c/span\u003e\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e \u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e in \u003cspan style=\"color: #00ff00; text-decoration-color: #00ff00\"\u003e\u0026lt;cell line: 2\u0026gt;\u003c/span\u003e:\u003cspan style=\"color: #0000ff; text-decoration-color: #0000ff\"\u003e6\u003c/span\u003e                                                                              \u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #800000; text-decoration-color: #800000\"\u003e\u003c/span\u003e\n","\u003cspan style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\"\u003eNameError: \u003c/span\u003ename \u003cspan style=\"color: #008000; text-decoration-color: #008000\"\u003e'extract_first_string'\u003c/span\u003e is not defined\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m in \u001b[92m\u003ccell line: 2\u003e\u001b[0m:\u001b[94m6\u001b[0m                                                                              \u001b[31m\u001b[0m\n","\u001b[31m\u001b[0m\n","\u001b[1;91mNameError: \u001b[0mname \u001b[32m'extract_first_string'\u001b[0m is not defined\n"]},"metadata":{},"output_type":"display_data"}],"source":["results = {}\n","for i, df in enumerate(dfs, start = 7):\n","    name = unique_accessions[i]\n","    df = dfs[name]\n","    print(name)\n","    df['predict_from_withstop'] = df['predict_from_withstop'].apply(extract_first_string)\n","    score = train(df, name)\n","    results[name] = [score]  # Store score as a list\n","\n","    print(score)\n","\n","# Create a DataFrame from the results\n","results_df = pd.DataFrame(results)\n","\n","# Save the DataFrame to a CSV file\n","results_df.to_csv('/content/drive/MyDrive/NAEP_Comp/MathBert+CNN/Results_Cohen_' + str(name) + '.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1686078082102,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"EVKgXnz9pDa7"},"outputs":[],"source":["inputs = df.predict_from.values\n","labels = df.assigned_score.values\n","print(\"Train data size \", len(inputs))\n","print('* Original:  ', inputs[0])\n","# Print the sentence split into tokens.\n","print('* Tokenized: ', bert_tokenizer.tokenize(inputs[0]))\n","# Print the sentence mapped to token ids.\n","print('* Token IDs: ', bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(inputs[0])))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1686078082102,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"Aqx6YW99q8gy"},"outputs":[],"source":["# Set the maximum sequence length.\n","MAX_LEN = 100\n","\n","# Print BERTs special PAD token and its index in the vocabulary\n","print(f'Padding token: \"{bert_tokenizer.pad_token}\", ID: {bert_tokenizer.pad_token_id}')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1686078082105,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"x6WcuXgJrJsu"},"outputs":[],"source":["tokenized_inputs = bert_tokenizer(\n","    inputs.tolist(),          # Input text\n","    add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n","    padding='max_length',     # pad to a length specified by the max_length\n","    max_length=MAX_LEN,       # truncate all sentences longer than max_length\n","    return_tensors='pt',  \n","    truncation = True    # return everything we need as PyTorch tensors\n",")\n","\n","input_ids = tokenized_inputs['input_ids']\n","attention_masks = tokenized_inputs['attention_mask']\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', tokenized_inputs['input_ids'][0])\n","print('* Token IDs:', tokenized_inputs['attention_mask'][0])\n","print('* Tokenized:', bert_tokenizer.decode(tokenized_inputs['input_ids'][0]))\n","print('* Attention_mask', tokenized_inputs['attention_mask'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1686078082106,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"pxasOOJGrm7b"},"outputs":[],"source":["bert_train_dataloader, bert_validation_dataloader = train_valid_split(\n","    input_ids=input_ids,\n","    attention_masks=attention_masks,\n","    labels=labels,\n","    batch_size=32\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1686078082106,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"jcJOmr6nuRzC"},"outputs":[],"source":["from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1686078082106,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"WDnELZLaxpGE"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForPreTraining\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1686078082107,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"5luRBwJ-rtW9"},"outputs":[],"source":["mathbert = BertForSentenceClassification.from_pretrained(\n","    \"tbs17/MathBERT\",  # the name of the pretrained model\n","    num_labels=4,      # the number of classes in our downstream task \n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1686078082107,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"Ui1k7RbXrzih"},"outputs":[],"source":["# Model parameters visualization\n","params = list(mathbert.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:\u003c55} {:\u003e12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer Layer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:\u003c55} {:\u003e12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:\u003c55} {:\u003e12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1686078082108,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"2jy44_Acsqbo"},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1686078082108,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"lsU4GbfLsdR6"},"outputs":[],"source":["# About 2-3 seconds per epoch using GPU\n","mathbert_loss_vals, mathbert_eval_accs = train_model(\n","    model=mathbert,\n","    epochs=1,\n","    train_dataloader=bert_train_dataloader,\n","    validation_dataloader=bert_validation_dataloader\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1686078082109,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"5awWfKDPsgn0"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def plot_loss_and_acc(loss_vals, eval_accs):\n","    sns.set(style='darkgrid')\n","    sns.set(font_scale=1.5)\n","    plt.rcParams[\"figure.figsize\"] = (12,6)\n","    fig, ax1 = plt.subplots(1,1)\n","    ax1.plot(loss_vals, 'b-o', label = 'training loss')\n","    ax2 = ax1.twinx()\n","    ax2.plot(eval_accs, 'y-o', label = 'validation accuracy')\n","    ax2.set_title(\"Training loss and validation accuracy\")\n","    ax2.set_xlabel(\"Epoch\")\n","    ax1.set_ylabel(\"Loss\", color='b')\n","    ax2.set_ylabel(\"Accuracy\", color='y')\n","    ax1.tick_params(axis='y', rotation=0, labelcolor='b' )\n","    ax2.tick_params(axis='y', rotation=0, labelcolor='y' )\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1686078082110,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"vMbALkrG_ii_"},"outputs":[],"source":["plot_loss_and_acc(mathbert_loss_vals, mathbert_eval_accs)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1686078082110,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"BwQPp7xtAN2U"},"outputs":[],"source":["mathbert_frozen = BertForSentenceClassification.from_pretrained(\n","    \"tbs17/MathBERT\",  # the name of the pretrained model\n","    num_labels=4,      # the number of classes in our downstream task\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1686078082110,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"fbwGRQ4KAOEg"},"outputs":[],"source":["for name, param in mathbert_frozen.named_parameters():\n","\t# Only compute gradients for parameters of our\n","\t# newly added classifier. BERT will not be trained.\n","\tif 'classifier' not in name:\n","\t\tparam.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"aborted","timestamp":1686078082111,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"PENSfa_bAdn-"},"outputs":[],"source":["# About 1 second per epoch on GPU\n","mathbert_frozen_loss_vals, mathbert_frozen_eval_accs = train_model(\n","    model=mathbert_frozen,\n","    epochs=3, \n","    train_dataloader=bert_train_dataloader,\n","    validation_dataloader=bert_validation_dataloader\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"aborted","timestamp":1686078082111,"user":{"displayName":"Chandramani","userId":"15720194962338037264"},"user_tz":240},"id":"A6vbxaQUAgPK"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyObEQH8ZF5zHGXuGRTbPa5U","machine_shape":"hm","mount_file_id":"17TZKv94c88Qlwt1yogIjVWzLCf2drpEg","name":"","provenance":[{"file_id":"17TZKv94c88Qlwt1yogIjVWzLCf2drpEg","timestamp":1686074416017},{"file_id":"1wCKgzCkt05UTxYBwukYF52lh7YP6zity","timestamp":1685397985416}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"067869e7befd43e39a5ebd6444e85eb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f83fa845e2140d9adcbe1bf3636a7c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"17caae589aac4bb395928a53e4dfeb64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_533a3789bf36412fa9ad4dd516e627ae","IPY_MODEL_949b4bd733684b1f83d3796ee2c4fb9b","IPY_MODEL_796ea54e20b64c2bb2da549bcb7f9dcf"],"layout":"IPY_MODEL_0f83fa845e2140d9adcbe1bf3636a7c2"}},"1c4155b9fcdb4dda930555beee4d0a8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1cb0277356324cb2afa9f240ba7747f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fd0144cc62c43eb89453a9c74422a74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2330c390d8fb44b6abd98a7722380a06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"368943d9bf004cf2b220bdae6ec473df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4aec7bc5ff9c4909b2dc714a4995d449":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"526e3b60d06747018424a31ec2da41f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"533a3789bf36412fa9ad4dd516e627ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4aec7bc5ff9c4909b2dc714a4995d449","placeholder":"","style":"IPY_MODEL_1fd0144cc62c43eb89453a9c74422a74","value":"Map: 100%"}},"6454cb7c03ae4a0195406362fe087023":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_526e3b60d06747018424a31ec2da41f2","placeholder":"","style":"IPY_MODEL_e5fbcb135c2147aaa2606c7814fe2ed6","value":"Map: 100%"}},"770082dc611c4ca28d927b942b1fba6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"796ea54e20b64c2bb2da549bcb7f9dcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_770082dc611c4ca28d927b942b1fba6a","placeholder":"","style":"IPY_MODEL_368943d9bf004cf2b220bdae6ec473df","value":" 3911/3911 [00:11\u0026lt;00:00, 325.80 examples/s]"}},"7adc7b9409e743efb310db4fde221734":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81bd5c089a21468a95e778c9cf82dfe2","placeholder":"","style":"IPY_MODEL_ee73668ca5af4039ae1fb2edc8e448b9","value":" 15644/15644 [00:47\u0026lt;00:00, 319.18 examples/s]"}},"81bd5c089a21468a95e778c9cf82dfe2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"949b4bd733684b1f83d3796ee2c4fb9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cb0277356324cb2afa9f240ba7747f1","max":3911,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcc8e8d84ec64474b9e6d25458d7c4f2","value":3911}},"bb62e26a49f6482d9048823d8a96e176":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6454cb7c03ae4a0195406362fe087023","IPY_MODEL_c1137b7797154e42b0a3d8ac4801e0ab","IPY_MODEL_7adc7b9409e743efb310db4fde221734"],"layout":"IPY_MODEL_2330c390d8fb44b6abd98a7722380a06"}},"bcc8e8d84ec64474b9e6d25458d7c4f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1137b7797154e42b0a3d8ac4801e0ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_067869e7befd43e39a5ebd6444e85eb6","max":15644,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c4155b9fcdb4dda930555beee4d0a8f","value":15644}},"e5fbcb135c2147aaa2606c7814fe2ed6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee73668ca5af4039ae1fb2edc8e448b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}